"""
å›¾ç‰‡è¯†åˆ«æ ¸å¿ƒæ¨¡å—
è´Ÿè´£å¾®ä¿¡é€šè¯æˆªå›¾çš„è¯†åˆ«ã€ä¿¡æ¯æå–å’Œæ•°æ®å¤„ç†
"""

import asyncio
import json
import logging
import re
from datetime import datetime, date
from typing import List, Dict, Any, Optional, Tuple
import openai
from config import IMAGE_RECOGNITION_CONFIG
from image_utils import optimize_image_for_llm, encode_image_to_base64, validate_image_format

logger = logging.getLogger(__name__)

# é…ç½®OpenAIå®¢æˆ·ç«¯ç”¨äºå›¾ç‰‡è¯†åˆ«
def get_image_recognition_client():
    """è·å–é…ç½®å¥½çš„OpenAIå®¢æˆ·ç«¯"""
    return openai.OpenAI(
        api_key=IMAGE_RECOGNITION_CONFIG["api_key"],
        base_url=IMAGE_RECOGNITION_CONFIG["api_base"]
    )

def create_image_recognition_prompt() -> str:
    """
    åˆ›å»ºä¸“é—¨ç”¨äºå¾®ä¿¡é€šè¯æˆªå›¾è¯†åˆ«çš„æç¤ºè¯
    
    Returns:
        ç»“æ„åŒ–çš„æç¤ºè¯
    """
    return """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾ç‰‡è¯†åˆ«åŠ©æ‰‹ï¼Œä¸“é—¨è¯†åˆ«å¾®ä¿¡èŠå¤©æˆªå›¾ä¸­çš„é€šè¯ä¿¡æ¯ã€‚

è¯·ä»”ç»†åˆ†æè¿™å¼ å¾®ä¿¡èŠå¤©æˆªå›¾ï¼Œæå–æ‰€æœ‰çš„é€šè¯è®°å½•ä¿¡æ¯ã€‚

**éœ€è¦æå–çš„ä¿¡æ¯ï¼š**
1. é€šè¯æ—¶é•¿ï¼ˆæ ¼å¼å¦‚ï¼š"é€šè¯æ—¶é•¿ 01:39"ã€"é€šè¯æ—¶é•¿ 1:23"ç­‰ï¼‰
2. è”ç³»äººä¿¡æ¯ï¼ˆèŠå¤©å¯¹è±¡çš„åç§°æˆ–å¤‡æ³¨ï¼‰
3. é€šè¯æ—¶é—´ï¼ˆå¦‚æœå¯è§ï¼Œæ ¼å¼å¦‚ï¼š"6æœˆ16æ—¥ ä¸‹åˆ15:46"ï¼‰
4. å…¬å¸ä¿¡æ¯ï¼ˆå¦‚æœèŠå¤©å†…å®¹ä¸­æåˆ°å…¬å¸åç§°ï¼‰

**é‡è¦è§„åˆ™ï¼š**
- é€šè¯æ—¶é•¿â‰¥60ç§’çš„ä¸ºæœ‰æ•ˆé€šè¯ï¼Œ<60ç§’çš„ä¸ºæ— æ•ˆé€šè¯
- ä¸€å¼ å›¾å¯èƒ½åŒ…å«å¤šæ¡é€šè¯è®°å½•ï¼Œè¯·å…¨éƒ¨æå–
- å¦‚æœçœ‹ä¸æ¸…æ¥šæŸäº›ä¿¡æ¯ï¼Œè¯·æ ‡è®°ä¸º"æœªçŸ¥"
- æ—¶é—´æ ¼å¼è¦è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼

**è¿”å›æ ¼å¼ï¼ˆä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼ï¼‰ï¼š**
```json
{
    "success": true,
    "total_calls_found": 2,
    "calls": [
        {
            "contact_info": "åæ–‡è´¸æ˜“ / HELI Xå£³ç‰Œ",
            "duration_text": "01:39",
            "duration_seconds": 99,
            "is_effective": true,
            "call_time": "6æœˆ16æ—¥ ä¸‹åˆ15:46",
            "call_date": "2024-06-16",
            "company_name": "åæ–‡è´¸æ˜“",
            "additional_info": "HELI Xå£³ç‰Œ å–œåŠ›ï¼ˆéƒ‘å·ï¼‰"
        }
    ],
    "error_message": null
}
```

å¦‚æœè¯†åˆ«å¤±è´¥ï¼Œè¿”å›ï¼š
```json
{
    "success": false,
    "total_calls_found": 0,
    "calls": [],
    "error_message": "è¯†åˆ«å¤±è´¥çš„å…·ä½“åŸå› "
}
```

ç°åœ¨è¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼š
"""

def parse_duration_to_seconds(duration_text: str) -> Optional[int]:
    """
    è§£æé€šè¯æ—¶é•¿æ–‡æœ¬ä¸ºç§’æ•°
    
    Args:
        duration_text: æ—¶é•¿æ–‡æœ¬ï¼Œå¦‚ "01:39", "1:23", "00:45"
    
    Returns:
        æ€»ç§’æ•°ï¼Œè§£æå¤±è´¥è¿”å›None
    """
    try:
        # æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤"é€šè¯æ—¶é•¿"ç­‰å‰ç¼€
        duration_text = re.sub(r'é€šè¯æ—¶é•¿\s*', '', duration_text)
        duration_text = duration_text.strip()
        
        # è§£æ MM:SS æ ¼å¼
        if ':' in duration_text:
            parts = duration_text.split(':')
            if len(parts) == 2:
                minutes = int(parts[0])
                seconds = int(parts[1])
                return minutes * 60 + seconds
        
        # å¦‚æœåªæ˜¯æ•°å­—ï¼Œå‡è®¾æ˜¯ç§’
        if duration_text.isdigit():
            return int(duration_text)
            
        return None
        
    except (ValueError, IndexError) as e:
        logger.warning(f"è§£ææ—¶é•¿å¤±è´¥: {duration_text}, é”™è¯¯: {e}")
        return None

def parse_call_date(call_time_text: str, current_year: int = None) -> Optional[str]:
    """
    è§£æé€šè¯æ—¶é—´æ–‡æœ¬ä¸ºæ ‡å‡†æ—¥æœŸæ ¼å¼
    
    Args:
        call_time_text: æ—¶é—´æ–‡æœ¬ï¼Œå¦‚ "6æœˆ16æ—¥ ä¸‹åˆ15:46", "ä»Šå¤© ä¸Šåˆ10:30"
        current_year: å½“å‰å¹´ä»½ï¼Œé»˜è®¤ä¸ºä»Šå¹´
    
    Returns:
        æ ‡å‡†æ—¥æœŸæ ¼å¼ YYYY-MM-DDï¼Œè§£æå¤±è´¥è¿”å›None
    """
    try:
        if current_year is None:
            current_year = datetime.now().year
        
        # å¤„ç†"ä»Šå¤©"ã€"æ˜¨å¤©"ç­‰ç›¸å¯¹æ—¶é—´
        today = date.today()
        if "ä»Šå¤©" in call_time_text:
            return today.strftime("%Y-%m-%d")
        elif "æ˜¨å¤©" in call_time_text:
            yesterday = date(today.year, today.month, today.day - 1) if today.day > 1 else date(today.year, today.month - 1, 28)
            return yesterday.strftime("%Y-%m-%d")
        
        # è§£æå…·ä½“æ—¥æœŸï¼Œå¦‚ "6æœˆ16æ—¥"
        month_day_match = re.search(r'(\d{1,2})æœˆ(\d{1,2})æ—¥', call_time_text)
        if month_day_match:
            month = int(month_day_match.group(1))
            day = int(month_day_match.group(2))
            return f"{current_year:04d}-{month:02d}-{day:02d}"
        
        # å¦‚æœåŒ…å«å¹´ä»½ï¼Œå¦‚ "2024å¹´6æœˆ16æ—¥"
        year_month_day_match = re.search(r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', call_time_text)
        if year_month_day_match:
            year = int(year_month_day_match.group(1))
            month = int(year_month_day_match.group(2))
            day = int(year_month_day_match.group(3))
            return f"{year:04d}-{month:02d}-{day:02d}"
        
        return None
        
    except (ValueError, AttributeError) as e:
        logger.warning(f"è§£ææ—¥æœŸå¤±è´¥: {call_time_text}, é”™è¯¯: {e}")
        return None

async def extract_call_info_from_image(image_content: bytes, filename: str) -> Dict[str, Any]:
    """
    ä»å•å¼ å›¾ç‰‡ä¸­æå–é€šè¯ä¿¡æ¯
    
    Args:
        image_content: å›¾ç‰‡å­—èŠ‚æ•°æ®
        filename: å›¾ç‰‡æ–‡ä»¶å
    
    Returns:
        æå–ç»“æœå­—å…¸
    """
    try:
        # ä¼˜åŒ–å›¾ç‰‡
        optimized_content = optimize_image_for_llm(image_content)
        base64_image = encode_image_to_base64(optimized_content)
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = get_image_recognition_client()
        
        # è°ƒç”¨LLMè¿›è¡Œå›¾ç‰‡è¯†åˆ«
        response = await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: client.chat.completions.create(
                model=IMAGE_RECOGNITION_CONFIG["model_name"],
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": create_image_recognition_prompt()
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{base64_image}"
                                }
                            }
                        ]
                    }
                ],
                temperature=IMAGE_RECOGNITION_CONFIG["temperature"],
                max_tokens=1000
            )
        )
        
        # è§£æå“åº”
        response_text = response.choices[0].message.content
        logger.info(f"LLMå“åº” ({filename}): {response_text}")
        
        # å°è¯•è§£æJSONå“åº”
        try:
            # æå–JSONéƒ¨åˆ†ï¼ˆå»é™¤å¯èƒ½çš„ä»£ç å—æ ‡è®°ï¼‰
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æ‰¾åˆ°JSONå¯¹è±¡
                json_text = response_text.strip()
            
            result = json.loads(json_text)
            
            # éªŒè¯å’Œè¡¥å……æ•°æ®
            if result.get("success", False) and result.get("calls"):
                validated_calls = []
                for call in result["calls"]:
                    # è§£ææ—¶é•¿
                    duration_seconds = parse_duration_to_seconds(call.get("duration_text", ""))
                    if duration_seconds is not None:
                        call["duration_seconds"] = duration_seconds
                        call["is_effective"] = duration_seconds >= 60
                    
                    # è§£ææ—¥æœŸ
                    call_date = parse_call_date(call.get("call_time", ""))
                    if call_date:
                        call["call_date"] = call_date
                    
                    # æ·»åŠ æºå›¾ç‰‡æ–‡ä»¶å
                    call["source_image_filename"] = filename
                    
                    validated_calls.append(call)
                
                result["calls"] = validated_calls
                result["total_calls_found"] = len(validated_calls)
                
            return {
                "status": "success",
                "filename": filename,
                "result": result
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥ ({filename}): {e}")
            return {
                "status": "error",
                "filename": filename,
                "error": f"LLMå“åº”æ ¼å¼é”™è¯¯: {str(e)}",
                "raw_response": response_text
            }
            
    except Exception as e:
        logger.error(f"å›¾ç‰‡è¯†åˆ«å¤±è´¥ ({filename}): {e}")
        return {
            "status": "error",
            "filename": filename,
            "error": str(e)
        }

async def process_image_batch(uploaded_images: List[Any], progress_callback=None) -> Dict[str, Any]:
    """
    æ‰¹é‡å¤„ç†å›¾ç‰‡ï¼Œæå–é€šè¯ä¿¡æ¯
    
    Args:
        uploaded_images: Streamlitä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
        progress_callback: è¿›åº¦æ›´æ–°å›è°ƒå‡½æ•°
    
    Returns:
        æ‰¹å¤„ç†ç»“æœ
    """
    total_images = len(uploaded_images)
    successful_results = []
    failed_results = []
    all_calls = []
    
    logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {total_images} å¼ å›¾ç‰‡")
    
    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
    tasks = []
    for i, image_file in enumerate(uploaded_images):
        # éªŒè¯å›¾ç‰‡æ ¼å¼
        is_valid, error_msg = validate_image_format(image_file)
        if not is_valid:
            failed_results.append({
                "filename": image_file.name,
                "error": error_msg
            })
            continue
        
        # åˆ›å»ºå¤„ç†ä»»åŠ¡
        image_content = image_file.getvalue()
        task = extract_call_info_from_image(image_content, image_file.name)
        tasks.append((i, task))
    
    # æ‰§è¡Œå¼‚æ­¥å¤„ç†
    for i, task in tasks:
        try:
            # æ›´æ–°è¿›åº¦
            if progress_callback:
                progress = (i + 1) / total_images
                progress_callback(progress, f"æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{total_images} å¼ å›¾ç‰‡...")
            
            result = await task
            
            if result["status"] == "success":
                successful_results.append(result)
                # æ”¶é›†æ‰€æœ‰é€šè¯è®°å½•
                if result["result"].get("calls"):
                    all_calls.extend(result["result"]["calls"])
            else:
                failed_results.append({
                    "filename": result["filename"],
                    "error": result["error"]
                })
                
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
            failed_results.append({
                "filename": f"task_{i}",
                "error": str(e)
            })
    
    # ç»Ÿè®¡ç»“æœ
    total_calls = len(all_calls)
    effective_calls = sum(1 for call in all_calls if call.get("is_effective", False))
    
    summary = {
        "total_images": total_images,
        "successful_images": len(successful_results),
        "failed_images": len(failed_results),
        "total_calls_found": total_calls,
        "effective_calls_found": effective_calls,
        "all_calls": all_calls,
        "successful_results": successful_results,
        "failed_results": failed_results
    }
    
    logger.info(f"æ‰¹å¤„ç†å®Œæˆ: {summary}")
    return summary

def prepare_database_update_data(processing_results: Dict[str, Any], salesperson_id: int) -> Dict[str, Any]:
    """
    å‡†å¤‡æ•°æ®åº“æ›´æ–°æ‰€éœ€çš„æ•°æ®ï¼Œæ˜ å°„åˆ°call_detailsè¡¨æ ¼å¼
    
    Args:
        processing_results: å›¾ç‰‡å¤„ç†ç»“æœ
        salesperson_id: é”€å”®äººå‘˜ID
    
    Returns:
        æ•°æ®åº“æ›´æ–°æ•°æ®
    """
    all_calls = processing_results.get("all_calls", [])
    
    # å‡†å¤‡ call_details æ ¼å¼çš„æ•°æ®åˆ—è¡¨
    call_details_list = []
    
    for call in all_calls:
        # æå–ç”µè¯å·ç ï¼ˆä»é™„åŠ ä¿¡æ¯ä¸­ï¼‰
        phone_number = extract_phone_from_text(call.get("additional_info", ""))
        
        # æ ¼å¼åŒ–é€šè¯æ—¶é—´ä¿¡æ¯
        call_time_info = format_call_time_info(call)
        
        # æ ¼å¼åŒ–é€šè¯ç»Ÿè®¡ä¿¡æ¯
        call_statistics = format_call_statistics(call)
        
        # æ˜ å°„åˆ° call_details è¡¨æ ¼å¼
        call_detail = {
            'original_filename': call.get('source_image_filename', 'æœªçŸ¥å›¾ç‰‡'),
            'company_name': call.get('company_name', '').strip() or None,
            'contact_person': call.get('contact_info', '').strip() or None,
            'phone_number': phone_number,
            'conversation_text': call_time_info,  # å­˜å‚¨é€šè¯æ—¶é—´ä¿¡æ¯
            'analysis_text': call_statistics,    # å­˜å‚¨é€šè¯ç»Ÿè®¡ä¿¡æ¯
            'score': None,  # å›¾ç‰‡æ¿å—ä¸ä½¿ç”¨è¯„åˆ†å­—æ®µ
            'is_effective': call.get('is_effective', False),
            'suggestions': None,  # å›¾ç‰‡è¯†åˆ«æš‚ä¸ç”Ÿæˆå»ºè®®
            'record_type': 'image'
        }
        
        call_details_list.append(call_detail)
    
    # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ç”¨äºç»Ÿè®¡ï¼‰
    daily_stats = {}
    for call in all_calls:
        call_date = call.get("call_date")
        if not call_date:
            call_date = date.today().strftime("%Y-%m-%d")  # å¦‚æœæ²¡æœ‰æ—¥æœŸï¼Œä½¿ç”¨ä»Šå¤©
        
        if call_date not in daily_stats:
            daily_stats[call_date] = {
                "total_calls": 0,
                "effective_calls": 0,
                "calls_details": []
            }
        
        daily_stats[call_date]["total_calls"] += 1
        if call.get("is_effective", False):
            daily_stats[call_date]["effective_calls"] += 1
        
        daily_stats[call_date]["calls_details"].append(call)
    
    return {
        "salesperson_id": salesperson_id,
        "call_details_list": call_details_list,  # æ–°å¢ï¼šcall_detailsæ ¼å¼çš„æ•°æ®
        "daily_stats": daily_stats,
        "total_images_processed": processing_results["successful_images"],
        "total_calls_found": processing_results["total_calls_found"],
        "total_effective_calls": processing_results["effective_calls_found"],
        "processing_errors": processing_results["failed_results"]
    }

def extract_phone_from_text(text: str) -> Optional[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–ç”µè¯å·ç 
    
    Args:
        text: è¦æœç´¢çš„æ–‡æœ¬
    
    Returns:
        æå–åˆ°çš„ç”µè¯å·ç ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿”å›None
    """
    if not text:
        return None
    
    import re
    # åŒ¹é…ä¸­å›½æ‰‹æœºå·å’Œå›ºå®šç”µè¯å·ç 
    phone_patterns = [
        r'1[3-9]\d{9}',          # æ‰‹æœºå·
        r'0\d{2,3}-?\d{7,8}',    # å›ºå®šç”µè¯
        r'\d{3}-?\d{8}',         # ç®€åŒ–å›ºå®šç”µè¯
    ]
    
    for pattern in phone_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group()
    
    return None

def format_call_time_info(call_data: Dict[str, Any]) -> Optional[str]:
    """
    æ ¼å¼åŒ–é€šè¯æ—¶é—´ä¿¡æ¯å­˜å‚¨åˆ°conversation_textå­—æ®µ
    
    Args:
        call_data: é€šè¯æ•°æ®
    
    Returns:
        æ ¼å¼åŒ–çš„é€šè¯æ—¶é—´ä¿¡æ¯
    """
    info_parts = []
    
    if call_data.get('call_time'):
        info_parts.append(f"é€šè¯æ—¶é—´: {call_data['call_time']}")
    if call_data.get('call_date'):
        info_parts.append(f"é€šè¯æ—¥æœŸ: {call_data['call_date']}")
    
    return '\n'.join(info_parts) if info_parts else None

def format_call_statistics(call_data: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–é€šè¯ç»Ÿè®¡ä¿¡æ¯å­˜å‚¨åˆ°analysis_textå­—æ®µ
    
    Args:
        call_data: é€šè¯æ•°æ®
    
    Returns:
        æ ¼å¼åŒ–çš„é€šè¯ç»Ÿè®¡ä¿¡æ¯
    """
    stats = []
    stats.append(f"é€šè¯æ—¶é•¿: {call_data.get('duration_text', 'æœªçŸ¥')}")
    stats.append(f"æ—¶é•¿ç§’æ•°: {call_data.get('duration_seconds', 0)}")
    stats.append(f"æ˜¯å¦æœ‰æ•ˆ: {'æ˜¯' if call_data.get('is_effective') else 'å¦'}")
    
    if call_data.get('additional_info'):
        stats.append(f"é™„åŠ ä¿¡æ¯: {call_data['additional_info']}")
    
    return '\n'.join(stats)

# å¯¼å‡ºä¸»è¦å‡½æ•°
__all__ = [
    "extract_call_info_from_image",
    "process_image_batch", 
    "prepare_database_update_data",
    "parse_duration_to_seconds",
    "parse_call_date",
    "check_image_duplicates",  # æ–°å¢ï¼šå›¾ç‰‡å»é‡æ£€æŸ¥
    "filter_duplicate_images",  # æ–°å¢ï¼šè¿‡æ»¤é‡å¤å›¾ç‰‡
    "smart_duplicate_detection",  # æ–°å¢ï¼šæ™ºèƒ½å»é‡æ£€æµ‹
    "calculate_similarity"  # æ–°å¢ï¼šç›¸ä¼¼åº¦è®¡ç®—
]

# æ™ºèƒ½å»é‡é…ç½®
DUPLICATE_DETECTION_WEIGHTS = {
    "call_time_match": 0.5,      # é€šè¯æ—¶é—´åŒ¹é… (50%æƒé‡)
    "call_duration_match": 0.3,  # é€šè¯æ—¶é•¿åŒ¹é… (30%æƒé‡)
    "contact_name_match": 0.1,   # è”ç³»äººåŒ¹é… (10%æƒé‡)
    "company_name_match": 0.1    # å…¬å¸åç§°åŒ¹é… (10%æƒé‡)
}

DUPLICATE_THRESHOLD = 0.7  # â‰¥0.7è‡ªåŠ¨è·³è¿‡ï¼Œ<0.7æ­£å¸¸å¤„ç†

def calculate_time_similarity(time1: Optional[str], time2: Optional[str]) -> float:
    """
    è®¡ç®—é€šè¯æ—¶é—´ç›¸ä¼¼åº¦
    
    Args:
        time1: é€šè¯æ—¶é—´1 (æ ¼å¼å¯èƒ½å¤šæ ·)
        time2: é€šè¯æ—¶é—´2
    
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    if not time1 or not time2:
        return 0.0
    
    try:
        from datetime import datetime
        import re
        
        # æ ‡å‡†åŒ–æ—¶é—´å­—ç¬¦ä¸²ï¼šç§»é™¤å¤šä½™ç©ºæ ¼ã€ç»Ÿä¸€æ ¼å¼
        time1 = re.sub(r'\s+', ' ', time1.strip())  # å°†å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        time2 = re.sub(r'\s+', ' ', time2.strip())
        
        # å¤„ç†"ä¸Šåˆ/ä¸‹åˆ"å‰åçš„ç©ºæ ¼ä¸ä¸€è‡´é—®é¢˜
        time1 = re.sub(r'(ä¸Šåˆ|ä¸‹åˆ|AM|PM)\s*', r'\1', time1)  # ç»Ÿä¸€ç§»é™¤æ—¶æ®µåçš„ç©ºæ ¼
        time2 = re.sub(r'(ä¸Šåˆ|ä¸‹åˆ|AM|PM)\s*', r'\1', time2)
        
        # ä»conversation_textä¸­æå–å…·ä½“çš„æ—¶é—´éƒ¨åˆ†
        # å¤„ç†æ ¼å¼ï¼š"é€šè¯æ—¶é—´: 6æœˆ24æ—¥ ä¸Šåˆ11:14\né€šè¯æ—¥æœŸ: 2025-06-24"
        time1_match = re.search(r'é€šè¯æ—¶é—´[:ï¼š]\s*(.+?)(?:\n|$)', time1)
        if time1_match:
            time1 = time1_match.group(1).strip()
        
        time2_match = re.search(r'é€šè¯æ—¶é—´[:ï¼š]\s*(.+?)(?:\n|$)', time2)
        if time2_match:
            time2 = time2_match.group(1).strip()
        
        # å¦‚æœå®Œå…¨ç›¸åŒï¼ˆæ ‡å‡†åŒ–åï¼‰ï¼Œè¿”å›1.0
        if time1 == time2:
            return 1.0
        
        # å°è¯•è§£ææ—¶é—´
        # æå–æ—¥æœŸå’Œæ—¶é—´éƒ¨åˆ†
        def parse_datetime(time_str):
            # å¢åŠ å¯¹ä¸­æ–‡æ—¶é—´æ ¼å¼çš„æ”¯æŒ
            patterns = [
                # ä¸­æ–‡æ ¼å¼ï¼š6æœˆ24æ—¥ ä¸Šåˆ11:14
                r'(\d{1,2}æœˆ\d{1,2}æ—¥)\s*(ä¸Šåˆ|ä¸‹åˆ)?(\d{1,2}[:ï¼š]\d{2})',
                # æ ‡å‡†æ ¼å¼ï¼š2025-06-24 11:14
                r'(\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?)\s*(\d{1,2}[:ï¼š]\d{2})',
                # å…¶ä»–æ ¼å¼
                r'(\d{4}-\d{2}-\d{2})\s*(\d{2}:\d{2})',
                r'(\d{2}/\d{2})\s*(\d{2}:\d{2})'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, time_str)
                if match:
                    if len(match.groups()) == 3:  # ä¸­æ–‡æ ¼å¼
                        date_part = match.group(1)
                        am_pm = match.group(2) or ''
                        time_part = match.group(3)
                        return date_part, f"{am_pm}{time_part}"
                    else:
                        date_part = match.group(1)
                        time_part = match.group(2)
                        return date_part, time_part
            
            return None, None
        
        date1, time1_only = parse_datetime(time1)
        date2, time2_only = parse_datetime(time2)
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿›è¡Œæ›´æ™ºèƒ½çš„å­—ç¬¦ä¸²æ¯”è¾ƒ
        if not (date1 and time1_only and date2 and time2_only):
            # ç¡®ä¿time1å’Œtime2ä¸æ˜¯None
            if time1 is None or time2 is None:
                return 0.0
                
            # ç§»é™¤æ‰€æœ‰ç©ºæ ¼åæ¯”è¾ƒ
            time1_no_space = time1.replace(' ', '')
            time2_no_space = time2.replace(' ', '')
            if time1_no_space == time2_no_space:
                return 0.95  # åªæ˜¯ç©ºæ ¼å·®å¼‚ï¼Œç»™é«˜åˆ†
            
            # æ£€æŸ¥æ˜¯å¦åªæ˜¯ç»†å¾®å·®å¼‚
            if len(time1) == len(time2):
                diff_count = sum(1 for a, b in zip(time1, time2) if a != b)
                if diff_count <= 2:  # åªæœ‰1-2ä¸ªå­—ç¬¦ä¸åŒ
                    return 0.8
            
            return 0.0
        
        # æ—¥æœŸä¸åŒç›´æ¥è¿”å›0
        if date1 != date2:
            return 0.0
        
        # æ¯”è¾ƒæ—¶é—´éƒ¨åˆ†ï¼ˆå¿½ç•¥ä¸Šåˆ/ä¸‹åˆçš„æ ¼å¼å·®å¼‚ï¼‰
        # ç»Ÿä¸€æ ¼å¼ï¼šç§»é™¤æ—¶æ®µæ ‡è®°ï¼Œåªæ¯”è¾ƒæ—¶é—´
        time1_clean = re.sub(r'(ä¸Šåˆ|ä¸‹åˆ|AM|PM)', '', time1_only).strip()
        time2_clean = re.sub(r'(ä¸Šåˆ|ä¸‹åˆ|AM|PM)', '', time2_only).strip()
        
        if time1_clean == time2_clean:
            return 1.0
        
        # è§£ææ—¶é—´éƒ¨åˆ†
        try:
            h1, m1 = map(int, time1_clean.replace('ï¼š', ':').split(':'))
            h2, m2 = map(int, time2_clean.replace('ï¼š', ':').split(':'))
            
            # è®¡ç®—æ—¶é—´å·®ï¼ˆåˆ†é’Ÿï¼‰
            minutes1 = h1 * 60 + m1
            minutes2 = h2 * 60 + m2
            diff_minutes = abs(minutes1 - minutes2)
            
            # æ ¹æ®æ—¶é—´å·®è®¡ç®—ç›¸ä¼¼åº¦
            if diff_minutes == 0:
                return 1.0
            elif diff_minutes <= 3:
                return 0.95
            elif diff_minutes <= 5:
                return 0.9
            elif diff_minutes <= 10:
                return 0.7
            elif diff_minutes <= 15:
                return 0.5
            else:
                return 0.2
        except:
            # æ—¶é—´è§£æå¤±è´¥ï¼Œä½†æ—¥æœŸç›¸åŒï¼Œç»™ä¸€ä¸ªä¸­ç­‰åˆ†æ•°
            return 0.5
            
    except Exception as e:
        logger.error(f"è®¡ç®—æ—¶é—´ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
        return 0.0

def calculate_duration_similarity(duration1: Optional[int], duration2: Optional[int]) -> float:
    """
    è®¡ç®—é€šè¯æ—¶é•¿ç›¸ä¼¼åº¦
    
    Args:
        duration1: é€šè¯æ—¶é•¿1ï¼ˆç§’ï¼‰
        duration2: é€šè¯æ—¶é•¿2ï¼ˆç§’ï¼‰
    
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    if duration1 is None or duration2 is None:
        return 0.0
    
    # è®¡ç®—æ—¶é•¿å·®ï¼ˆç§’ï¼‰
    diff_seconds = abs(duration1 - duration2)
    
    # æ ¹æ®æ—¶é•¿å·®è®¡ç®—ç›¸ä¼¼åº¦
    if diff_seconds == 0:
        return 1.0
    elif diff_seconds <= 3:
        return 0.95
    elif diff_seconds <= 5:
        return 0.9
    elif diff_seconds <= 10:
        return 0.8
    elif diff_seconds <= 15:
        return 0.6
    elif diff_seconds <= 30:
        return 0.4
    else:
        return 0.1

def calculate_text_similarity(text1: Optional[str], text2: Optional[str]) -> float:
    """
    è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆè”ç³»äººæˆ–å…¬å¸åç§°ï¼‰
    
    Args:
        text1: æ–‡æœ¬1
        text2: æ–‡æœ¬2
    
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    # å¤„ç†ç©ºå€¼æƒ…å†µ
    if not text1 and not text2:
        return 0.5  # éƒ½ä¸ºç©ºï¼Œç»™ä¸­ç­‰åˆ†æ•°
    if not text1 or not text2:
        return 0.5  # ä¸€ä¸ªä¸ºç©ºï¼Œç»™ä¸­ç­‰åˆ†æ•°
    
    # æ ‡å‡†åŒ–æ–‡æœ¬
    text1 = text1.strip().lower()
    text2 = text2.strip().lower()
    
    # å®Œå…¨åŒ¹é…
    if text1 == text2:
        return 1.0
    
    # åŒ…å«å…³ç³»
    if text1 in text2 or text2 in text1:
        return 0.8
    
    # éƒ¨åˆ†åŒ¹é…ï¼ˆè€ƒè™‘å§“æ°ç­‰ï¼‰
    if len(text1) > 0 and len(text2) > 0:
        if text1[0] == text2[0]:  # é¦–å­—ç›¸åŒï¼ˆå¦‚å§“æ°ï¼‰
            return 0.6
    
    # è®¡ç®—ç¼–è¾‘è·ç¦»ï¼ˆç®€åŒ–ç‰ˆï¼‰
    try:
        # ç®€å•çš„å­—ç¬¦é‡åˆåº¦
        common_chars = sum(1 for c in text1 if c in text2)
        similarity = common_chars / max(len(text1), len(text2))
        return min(0.5, similarity)
    except:
        return 0.0

def adjust_weights_for_missing_data(call1: Dict[str, Any], call2: Dict[str, Any]) -> Dict[str, float]:
    """
    æ ¹æ®ç¼ºå¤±æ•°æ®è°ƒæ•´æƒé‡
    
    Args:
        call1: é€šè¯è®°å½•1
        call2: é€šè¯è®°å½•2
    
    Returns:
        è°ƒæ•´åçš„æƒé‡å­—å…¸
    """
    weights = DUPLICATE_DETECTION_WEIGHTS.copy()
    
    # æ£€æŸ¥ç¼ºå¤±æƒ…å†µ
    contact_missing = not (call1.get('contact_person') and call2.get('contact_person'))
    company_missing = not (call1.get('company_name') and call2.get('company_name'))
    
    if contact_missing and company_missing:
        # ä¸¤ä¸ªéƒ½ç¼ºå¤±ï¼šæ—¶é—´æƒé‡æå‡åˆ°90%
        weights["call_time_match"] = 0.6
        weights["call_duration_match"] = 0.3
        weights["contact_name_match"] = 0.05
        weights["company_name_match"] = 0.05
    elif contact_missing:
        # è”ç³»äººç¼ºå¤±ï¼šæ—¶é—´æƒé‡æå‡5%
        weights["call_time_match"] = 0.55
        weights["call_duration_match"] = 0.3
        weights["contact_name_match"] = 0.05
        weights["company_name_match"] = 0.1
    elif company_missing:
        # å…¬å¸ç¼ºå¤±ï¼šæ—¶é—´æƒé‡æå‡5%
        weights["call_time_match"] = 0.55
        weights["call_duration_match"] = 0.3
        weights["contact_name_match"] = 0.1
        weights["company_name_match"] = 0.05
    
    return weights

def calculate_similarity(call1: Dict[str, Any], call2: Dict[str, Any]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªé€šè¯è®°å½•çš„ç›¸ä¼¼åº¦
    
    Args:
        call1: æ–°çš„é€šè¯è®°å½•ï¼ˆä»å›¾ç‰‡è¯†åˆ«ï¼‰
        call2: ç°æœ‰çš„é€šè¯è®°å½•ï¼ˆä»æ•°æ®åº“ï¼‰
    
    Returns:
        ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
    """
    # 1. æ—¶é—´ç›¸ä¼¼åº¦ (50%æƒé‡)
    time_sim = calculate_time_similarity(
        call1.get('call_time'),  # æ–°è®°å½•çš„æ—¶é—´
        call2.get('conversation_text')  # æ•°æ®åº“ä¸­çš„æ—¶é—´
    )
    
    # 2. æ—¶é•¿ç›¸ä¼¼åº¦ (30%æƒé‡)
    # ä»æ•°æ®åº“çš„analysis_textä¸­æå–æ—¶é•¿
    duration2 = extract_duration_from_analysis(call2.get('analysis_text', ''))
    duration_sim = calculate_duration_similarity(
        call1.get('duration_seconds'),
        duration2
    )
    
    # 3. è”ç³»äººç›¸ä¼¼åº¦ (10%æƒé‡)
    contact_sim = calculate_text_similarity(
        call1.get('contact_info'),  # ä½¿ç”¨contact_infoå­—æ®µ
        call2.get('contact_person')
    )
    
    # 4. å…¬å¸ç›¸ä¼¼åº¦ (10%æƒé‡)
    company_sim = calculate_text_similarity(
        call1.get('company_name'),
        call2.get('company_name')
    )
    
    # åŠ¨æ€æƒé‡è°ƒæ•´ï¼ˆå¤„ç†ç¼ºå¤±æ•°æ®ï¼‰
    # éœ€è¦è½¬æ¢call1çš„å­—æ®µåä»¥åŒ¹é…æƒé‡è°ƒæ•´å‡½æ•°
    call1_adjusted = {
        'contact_person': call1.get('contact_info'),
        'company_name': call1.get('company_name')
    }
    weights = adjust_weights_for_missing_data(call1_adjusted, call2)
    
    # åŠ æƒè®¡ç®—æ€»ç›¸ä¼¼åº¦
    total_similarity = (
        time_sim * weights["call_time_match"] +
        duration_sim * weights["call_duration_match"] +
        contact_sim * weights["contact_name_match"] +
        company_sim * weights["company_name_match"]
    )
    
    logger.debug(f"ç›¸ä¼¼åº¦è®¡ç®—è¯¦æƒ…: æ—¶é—´={time_sim:.2f}, æ—¶é•¿={duration_sim:.2f}, "
                f"è”ç³»äºº={contact_sim:.2f}, å…¬å¸={company_sim:.2f}, æ€»åˆ†={total_similarity:.2f}")
    
    return total_similarity

def extract_duration_from_analysis(analysis_text: str) -> Optional[int]:
    """
    ä»analysis_textä¸­æå–é€šè¯æ—¶é•¿ï¼ˆç§’ï¼‰
    
    Args:
        analysis_text: åˆ†ææ–‡æœ¬
    
    Returns:
        æ—¶é•¿ï¼ˆç§’ï¼‰æˆ–None
    """
    if not analysis_text:
        return None
    
    # å°è¯•åŒ¹é…å„ç§æ—¶é•¿æ ¼å¼
    patterns = [
        r'æ—¶é•¿ç§’æ•°[:ï¼š]\s*(\d+)',      # åŒ¹é… "æ—¶é•¿ç§’æ•°: 74"
        r'(\d+)\s*ç§’',                 # åŒ¹é… "74ç§’"
        r'é€šè¯æ—¶é•¿[:ï¼š]\s*(\d+)\s*ç§’',  # åŒ¹é… "é€šè¯æ—¶é•¿: 74ç§’"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, analysis_text)
        if match:
            return int(match.group(1))
    
    # å¦‚æœæ²¡æœ‰ç›´æ¥çš„ç§’æ•°ï¼Œå°è¯•è§£ææ—¶é•¿æ–‡æœ¬ï¼ˆå¦‚ "01:14"ï¼‰
    duration_pattern = r'é€šè¯æ—¶é•¿[:ï¼š]\s*(\d{1,2}):(\d{2})'
    match = re.search(duration_pattern, analysis_text)
    if match:
        minutes = int(match.group(1))
        seconds = int(match.group(2))
        return minutes * 60 + seconds
    
    return None

def smart_duplicate_detection(new_calls: List[Dict[str, Any]], 
                            existing_calls: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ™ºèƒ½å»é‡æ£€æµ‹
    
    Args:
        new_calls: æ–°è¯†åˆ«çš„é€šè¯åˆ—è¡¨
        existing_calls: æ•°æ®åº“ä¸­çš„ç°æœ‰é€šè¯è®°å½•
    
    Returns:
        {
            "processed_calls": [...],    # å°†è¦å¤„ç†çš„é€šè¯åˆ—è¡¨
            "skipped_calls": [...],      # è·³è¿‡çš„é‡å¤é€šè¯åˆ—è¡¨
            "skip_count": 3,             # è·³è¿‡æ•°é‡
            "process_count": 7           # å¤„ç†æ•°é‡
        }
    """
    processed_calls = []
    skipped_calls = []
    
    logger.info(f"ğŸ¤– å¼€å§‹æ™ºèƒ½å»é‡æ£€æµ‹: {len(new_calls)} ä¸ªæ–°è®°å½•, {len(existing_calls)} ä¸ªç°æœ‰è®°å½•")
    
    for new_call in new_calls:
        max_similarity = 0
        best_match = None
        
        # ä¸æ¯ä¸ªç°æœ‰è®°å½•æ¯”è¾ƒ
        for existing_call in existing_calls:
            similarity = calculate_similarity(new_call, existing_call)
            if similarity > max_similarity:
                max_similarity = similarity
                best_match = existing_call
        
        # æ ¹æ®ç›¸ä¼¼åº¦å†³å®šå¤„ç†æ–¹å¼
        if max_similarity >= DUPLICATE_THRESHOLD:
            # è‡ªåŠ¨è·³è¿‡
            skipped_calls.append({
                "call": new_call,
                "matched_call": best_match,
                "similarity": max_similarity
            })
            logger.info(f"ğŸ“Œ è·³è¿‡é‡å¤è®°å½•: {new_call.get('contact_info', 'æœªçŸ¥')} "
                       f"(ç›¸ä¼¼åº¦: {max_similarity:.2f})")
        else:
            # æ­£å¸¸å¤„ç†
            processed_calls.append(new_call)
    
    logger.info(f"âœ… æ™ºèƒ½å»é‡å®Œæˆ: å¤„ç† {len(processed_calls)} ä¸ª, è·³è¿‡ {len(skipped_calls)} ä¸ª")
    
    return {
        "processed_calls": processed_calls,
        "skipped_calls": skipped_calls,
        "skip_count": len(skipped_calls),
        "process_count": len(processed_calls)
    }

def check_image_duplicates(uploaded_images: List[Any], salesperson_id: int, db_manager) -> Dict[str, Any]:
    """
    æ£€æŸ¥ä¸Šä¼ å›¾ç‰‡çš„æ–‡ä»¶åé‡å¤æƒ…å†µ
    
    Args:
        uploaded_images: Streamlitä¸Šä¼ çš„å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
        salesperson_id: é”€å”®äººå‘˜ID
        db_manager: æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹
    
    Returns:
        å»é‡æ£€æŸ¥ç»“æœå­—å…¸
    """
    import logging
    
    logger = logging.getLogger(__name__)
    
    if not uploaded_images:
        return {
            "has_duplicates": False,
            "duplicates": [],
            "new_files": [],
            "duplicate_files": [],
            "clean_files": uploaded_images
        }
    
    # æå–æ–‡ä»¶ååˆ—è¡¨
    filenames = [img.name for img in uploaded_images]
    
    logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶åé‡å¤æƒ…å†µ")
    logger.info(f"   é”€å”®äººå‘˜ID: {salesperson_id}")
    logger.info(f"   å›¾ç‰‡æ–‡ä»¶æ•°: {len(filenames)}")
    logger.info(f"   æ–‡ä»¶åˆ—è¡¨: {filenames}")
    
    try:
        # è°ƒç”¨æ•°æ®åº“æ£€æŸ¥å‡½æ•°
        duplicate_result = db_manager.check_duplicate_filenames(
            salesperson_id=salesperson_id,
            filenames=filenames,
            days_back=30  # æ£€æŸ¥æœ€è¿‘30å¤©
        )
        
        duplicates = duplicate_result.get("duplicates", [])
        new_files = duplicate_result.get("new_files", [])
        
        # åˆ†ç¦»é‡å¤å’Œéé‡å¤çš„å›¾ç‰‡æ–‡ä»¶å¯¹è±¡
        duplicate_filenames = [dup["filename"] for dup in duplicates]
        duplicate_files = [img for img in uploaded_images if img.name in duplicate_filenames]
        clean_files = [img for img in uploaded_images if img.name in new_files]
        
        result = {
            "has_duplicates": len(duplicates) > 0,
            "duplicates": duplicates,  # é‡å¤æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯
            "new_files": new_files,   # æ–°æ–‡ä»¶ååˆ—è¡¨
            "duplicate_files": duplicate_files,  # é‡å¤çš„å›¾ç‰‡æ–‡ä»¶å¯¹è±¡
            "clean_files": clean_files,  # éé‡å¤çš„å›¾ç‰‡æ–‡ä»¶å¯¹è±¡
            "total_images": len(uploaded_images),
            "duplicate_count": len(duplicates),
            "new_count": len(new_files)
        }
        
        logger.info(f"âœ… å›¾ç‰‡å»é‡æ£€æŸ¥å®Œæˆ:")
        logger.info(f"   æ€»å›¾ç‰‡æ•°: {result['total_images']}")
        logger.info(f"   é‡å¤æ–‡ä»¶: {result['duplicate_count']} ä¸ª")
        logger.info(f"   æ–°æ–‡ä»¶: {result['new_count']} ä¸ª")
        
        if duplicates:
            logger.info(f"ğŸ“‹ é‡å¤æ–‡ä»¶è¯¦æƒ…:")
            for dup in duplicates:
                logger.info(f"   - {dup['filename']} (ä¸Šæ¬¡ä¸Šä¼ : {dup['last_upload_date']}, {dup['days_ago']}å¤©å‰)")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ å›¾ç‰‡å»é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
        # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œå°†æ‰€æœ‰æ–‡ä»¶éƒ½è§†ä¸ºæ–°æ–‡ä»¶
        return {
            "has_duplicates": False,
            "duplicates": [],
            "new_files": filenames,
            "duplicate_files": [],
            "clean_files": uploaded_images,
            "total_images": len(uploaded_images),
            "duplicate_count": 0,
            "new_count": len(uploaded_images),
            "error": str(e)
        }

def filter_duplicate_images(uploaded_images: List[Any], duplicate_result: Dict[str, Any], user_choice: str) -> List[Any]:
    """
    æ ¹æ®ç”¨æˆ·é€‰æ‹©è¿‡æ»¤é‡å¤å›¾ç‰‡
    
    Args:
        uploaded_images: åŸå§‹ä¸Šä¼ çš„å›¾ç‰‡åˆ—è¡¨
        duplicate_result: å»é‡æ£€æŸ¥ç»“æœ
        user_choice: ç”¨æˆ·é€‰æ‹© ('skip_duplicates', 'force_all', 'manual_select')
    
    Returns:
        è¿‡æ»¤åçš„å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
    """
    import logging
    
    logger = logging.getLogger(__name__)
    
    if user_choice == "skip_duplicates":
        # è·³è¿‡é‡å¤é¡¹ï¼Œåªå¤„ç†æ–°æ–‡ä»¶
        filtered_images = duplicate_result.get("clean_files", [])
        logger.info(f"ğŸ“ ç”¨æˆ·é€‰æ‹©è·³è¿‡é‡å¤é¡¹: å¤„ç† {len(filtered_images)} å¼ æ–°å›¾ç‰‡")
        
    elif user_choice == "force_all":
        # å¼ºåˆ¶å¤„ç†æ‰€æœ‰æ–‡ä»¶
        filtered_images = uploaded_images
        logger.info(f"ğŸ“ ç”¨æˆ·é€‰æ‹©å¼ºåˆ¶å¤„ç†æ‰€æœ‰æ–‡ä»¶: å¤„ç† {len(filtered_images)} å¼ å›¾ç‰‡")
        
    else:
        # é»˜è®¤æƒ…å†µï¼šå¦‚æœæ²¡æœ‰é‡å¤ï¼Œå¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼›å¦‚æœæœ‰é‡å¤ï¼Œéœ€è¦ç”¨æˆ·æ˜ç¡®é€‰æ‹©
        if not duplicate_result.get("has_duplicates", False):
            filtered_images = uploaded_images
            logger.info(f"ğŸ“ æ— é‡å¤æ–‡ä»¶: å¤„ç† {len(filtered_images)} å¼ å›¾ç‰‡")
        else:
            # æœ‰é‡å¤ä½†ç”¨æˆ·æœªé€‰æ‹©ï¼Œè¿”å›ç©ºåˆ—è¡¨ç­‰å¾…ç”¨æˆ·é€‰æ‹©
            filtered_images = []
            logger.info(f"ğŸ“ æœ‰é‡å¤æ–‡ä»¶ä½†ç”¨æˆ·æœªé€‰æ‹©å¤„ç†æ–¹å¼: æš‚åœå¤„ç†")
    
    return filtered_images 