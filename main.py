import streamlit as st
import os
import json
import time
import base64
import hashlib
import hmac
import urllib
import asyncio
import aiohttp
import logging
from typing import List, Dict
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import HumanMessage, SystemMessage
import pandas as pd
from io import BytesIO
import re
import openpyxl

# é…ç½®æ—¥å¿—è¾“å‡º
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s')

# è®¯é£APIé…ç½®
lfasr_host = 'https://raasr.xfyun.cn/v2/api'
api_upload = '/upload'
api_get_result = '/getResult'
appid = "8d2e895b"
secret_key = "8d5c02bd69345f504761da6b818b423f"

# è¯·æ±‚ç­¾åç”Ÿæˆ
def get_signa(appid, secret_key, ts):
    m2 = hashlib.md5()
    m2.update((appid + ts).encode('utf-8'))
    md5 = m2.hexdigest()
    md5 = bytes(md5, encoding='utf-8')
    signa = hmac.new(secret_key.encode('utf-8'), md5, hashlib.sha1).digest()
    signa = base64.b64encode(signa)
    signa = str(signa, 'utf-8')
    return signa

async def upload_file_async(session: aiohttp.ClientSession, file_path: str) -> Dict:
    """å¼‚æ­¥ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
    ts = str(int(time.time()))
    signa = get_signa(appid, secret_key, ts)
    file_len = os.path.getsize(file_path)
    file_name = os.path.basename(file_path)
    param_dict = {
        'appId': appid,
        'signa': signa,
        'ts': ts,
        'fileSize': file_len,
        'fileName': file_name,
        'duration': "200",
        'roleNum': 2,
        'roleType': 1
    }
    url = lfasr_host + api_upload + "?" + urllib.parse.urlencode(param_dict)
    with open(file_path, 'rb') as f:
        data = f.read()
    async with session.post(url, headers={"Content-type": "application/json"}, data=data) as response:
        result = await response.json()
        logging.debug(f"ä¸Šä¼ æ–‡ä»¶ {file_name} è¿”å›ç»“æœï¼š{result}")
        return {"file_path": file_path, "result": result}

async def upload_files_async(file_paths: List[str]) -> List[Dict]:
    """å¹¶å‘ä¸Šä¼ å¤šä¸ªæ–‡ä»¶"""
    async with aiohttp.ClientSession() as session:
        tasks = [upload_file_async(session, file_path) for file_path in file_paths]
        return await asyncio.gather(*tasks)

async def get_transcription_result_async(orderId: str) -> Dict:
    """
    å¼‚æ­¥è·å–è½¬å†™ç»“æœ
    """
    ts = str(int(time.time()))
    signa = get_signa(appid, secret_key, ts)
    param_dict = {
        'appId': appid,
        'signa': signa,
        'ts': ts,
        'orderId': orderId,
        'resultType': "transfer,predict"
    }
    url = lfasr_host + api_get_result + "?" + urllib.parse.urlencode(param_dict)
    status = 3
    async with aiohttp.ClientSession() as session:
        while status == 3:
            async with session.post(url, headers={"Content-type": "application/json"}) as response:
                result = await response.json()
            status = result['content']['orderInfo']['status']
            logging.debug(f"è½¬å†™APIè°ƒç”¨è¿”å›çŠ¶æ€: {status} (orderId: {orderId})")
            if status == 4:
                break
            await asyncio.sleep(5)
    return result

def merge_result_for_one_vad(result_vad):
    """è§„èŒƒåŒ–JSONæ–‡ä»¶ä¸ºå¯è¯»æ–‡æœ¬"""
    content = []
    for rt_dic in result_vad['st']['rt']:
        spk_str = 'spk' + str(3 - int(result_vad['st']['rl'])) + '##'
        for st_dic in rt_dic['ws']:
            for cw_dic in st_dic['cw']:
                for w in cw_dic['w']:
                    spk_str += w
        spk_str += '\n'
        content.append(spk_str)
    return content

def identify_roles(raw_text: str) -> dict:
    """
    ä½¿ç”¨LLMè¯†åˆ«å¯¹è¯ä¸­çš„è§’è‰²
    """
    lines = raw_text.strip().split('\n')
    sample_dialogue = '\n'.join(lines[:10])
    llm = ChatOpenAI(
        openai_api_key="sk-OdCoqKCvctCJaPHUF2Ea9eF9C01940D8Aa7cB82889EaE165",
        openai_api_base="https://api.pumpkinaigc.online/v1",
        model_name="gpt-4o",
        temperature=0.2
    )
    system_prompt = """
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¯¹è¯åˆ†æä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œè¯†åˆ«å‡ºspk1å’Œspk2å„è‡ªçš„è§’è‰²ï¼ˆé”€å”®è¿˜æ˜¯å®¢æˆ·ï¼‰ã€‚

    åˆ¤æ–­ä¾æ®ï¼š
    1. è¯´è¯æ–¹å¼å’Œè¯­æ°”ï¼ˆé”€å”®é€šå¸¸æ›´ä¸»åŠ¨ã€æ›´æ­£å¼ï¼‰
    2. æé—®æ–¹å¼ï¼ˆé”€å”®å€¾å‘äºå¼•å¯¼æ€§æé—®ï¼‰
    3. ä¸“ä¸šæœ¯è¯­çš„ä½¿ç”¨ï¼ˆé”€å”®æ›´å¯èƒ½ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼‰
    4. ä¿¡æ¯è·å–æ–¹å‘ï¼ˆé”€å”®å€¾å‘äºè·å–å®¢æˆ·éœ€æ±‚ä¿¡æ¯ï¼‰

    è¯·åªè¿”å›å¦‚ä¸‹æ ¼å¼çš„JSONï¼š
    {
        "spk1": "é”€å”®/å®¢æˆ·",
        "spk2": "é”€å”®/å®¢æˆ·",
        "confidence": "high/medium/low"
    }
    """
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"å¯¹è¯å†…å®¹ï¼š\n\n{sample_dialogue}")
    ])
    try:
        response = llm(prompt.format_messages())
        roles = json.loads(response.content)
        return roles
    except Exception as e:
        return {
            "spk1": "æœªçŸ¥è§’è‰²1",
            "spk2": "æœªçŸ¥è§’è‰²2",
            "confidence": "low"
        }

def format_conversation_with_roles(raw_text: str, roles: dict) -> str:
    """
    æ ¹æ®å·²æœ‰çš„è§’è‰²ä¿¡æ¯ï¼Œå°†åŸå§‹çš„spkæ ‡è®°æ–‡æœ¬è½¬æ¢ä¸ºæ›´è§„èŒƒçš„å¯¹è¯æ ¼å¼
    """
    lines = raw_text.strip().split('\n')
    formatted_lines = []
    current_speaker = None
    current_content = []
    for line in lines:
        if not line.strip() or '##' not in line:
            continue
        speaker, content = line.split('##', 1)
        content = content.strip()
        if not content or content.strip().replace('ã€', '').isdigit():
            continue
        speaker_role = roles.get(speaker, f"æœªçŸ¥è§’è‰²{speaker[-1]}")
        if speaker == current_speaker:
            current_content.append(content)
        else:
            if current_speaker and current_content:
                formatted_lines.append(f"{roles.get(current_speaker, f'æœªçŸ¥è§’è‰²{current_speaker[-1]}')}ï¼š{''.join(current_content)}")
            current_speaker = speaker
            current_content = [content]
    if current_speaker and current_content:
        formatted_lines.append(f"{roles.get(current_speaker, f'æœªçŸ¥è§’è‰²{current_speaker[-1]}')}ï¼š{''.join(current_content)}")
    formatted_text = '\n\n'.join(formatted_lines)
    return formatted_text

def analyze_conversation_with_roles(conversation_text: str, roles: dict) -> dict:
    """
    ä½¿ç”¨LLMå¯¹é€šè¯è®°å½•è¿›è¡Œåˆ†æï¼Œå¹¶ç»™å‡ºæ”¹è¿›å»ºè®®ï¼Œæ­¤å¤„ä¸å†è°ƒç”¨identify_rolesï¼Œè€Œæ˜¯ä½¿ç”¨ä¼ å…¥çš„roles
    """
    formatted_text = format_conversation_with_roles(conversation_text, roles)
    confidence_warning = ""
    if roles.get("confidence", "low") == "low":
        confidence_warning = "\n\n æ³¨æ„ï¼šç³»ç»Ÿå¯¹è¯´è¯è€…è§’è‰²çš„è¯†åˆ«å¯ä¿¡åº¦è¾ƒä½ï¼Œè¯·äººå·¥æ ¸å®ã€‚"
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é”€å”®é€šè¯åˆ†æä¸“å®¶ï¼Œè´Ÿè´£å¯¹é”€å”®å¯¹è¯è¿›è¡Œåˆ†æè¯„ä¼°ã€‚
    ä»¥ä¸‹æ˜¯å¯¹è¯è®°å½•ï¼Œå…¶ä¸­ {roles['spk1']} çš„å‘è¨€ä»¥ "{roles['spk1']}ï¼š" å¼€å¤´ï¼Œ{roles['spk2']} çš„å‘è¨€ä»¥ "{roles['spk2']}ï¼š" å¼€å¤´ã€‚

    <è§’è‰²æ ‡è¯†>
    {roles['spk1']}: {{ROLES_SPK1}}
    {roles['spk2']}: {{ROLES_SPK2}}
    </è§’è‰²æ ‡è¯†>

    è¯·æŒ‰ç…§ä»¥ä¸‹è¯„åˆ†æ ‡å‡†å¯¹é”€å”®å¯¹è¯è¿›è¡Œè¯„ä¼°ï¼š
    1. 30 ç§’è‡ªæˆ‘ä»‹ç»æ¸…æ™°åº¦ï¼ˆ30 åˆ†ï¼‰
        - æ˜¯å¦åŒ…å«å…¬å¸/ä¸ªäººæ ¸å¿ƒä»·å€¼
        - æ˜¯å¦æ§åˆ¶åœ¨ 30 ç§’ä¸­
        - æ˜¯å¦å»ºç«‹ä¸“ä¸šå¯ä¿¡å½¢è±¡
    2. å®¢æˆ·éœ€æ±‚æ´å¯Ÿï¼ˆ20 åˆ†ï¼‰
        - æ˜¯å¦æ˜ç¡®å®¢æˆ·è¡Œä¸šç±»å‹
        - æ˜¯å¦ç¡®è®¤ç°æœ‰éœ€æ±‚
        - æ˜¯å¦é‡åŒ–å®¢æˆ·ä¸šåŠ¡è§„æ¨¡
    3. SPIN ç—›ç‚¹æŒ–æ˜ï¼ˆ15 åˆ†ï¼‰
        - Situationï¼šæ˜¯å¦ç¡®è®¤ç°çŠ¶
        - Problemï¼šæ˜¯å¦å‘ç°é—®é¢˜
        - Implicationï¼šæ˜¯å¦é˜æ˜é—®é¢˜å½±å“
        - Need - Payoffï¼šæ˜¯å¦å¼•å¯¼è§£å†³æ–¹æ¡ˆéœ€æ±‚
    4. ä»·å€¼å±•ç¤ºèƒ½åŠ›ï¼ˆ15 åˆ†ï¼‰
        - æ˜¯å¦é’ˆå¯¹æ€§åŒ¹é…å®¢æˆ·éœ€æ±‚
        - æ˜¯å¦ä½¿ç”¨æ•°æ®/æ¡ˆä¾‹æ”¯æ’‘
        - æ˜¯å¦è¯´æ˜ROIæˆ–æˆæœ¬æ•ˆç›Š
    5. å†³ç­–æµç¨‹æŒæ¡ï¼ˆ10 åˆ†ï¼‰
        - æ˜¯å¦ç¡®è®¤é‡‡è´­å†³ç­–é˜¶æ®µ
        - æ˜¯å¦è¯†åˆ«å…³é”®å†³ç­–äºº
        - æ˜¯å¦äº†è§£é¢„ç®—å‘¨æœŸ
    6. åç»­è·Ÿè¿›é“ºå«ï¼ˆ10 åˆ†ï¼‰
        - æ˜¯å¦çº¦å®šå…·ä½“è·Ÿè¿›æ—¶é—´
        - æ˜¯å¦è®¾ç½®ä»·å€¼é”šç‚¹
        - æ˜¯å¦å–å¾—å®¢æˆ·æ‰¿è¯º

    è¯·æŒ‰ä»¥ä¸‹æµç¨‹æ‰§è¡Œåˆ†æï¼š
    1. é’ˆå¯¹æ¯ä¸ªè¯„åˆ†æ ‡å‡†ï¼Œå…ˆåœ¨[åˆ†æå†…å®¹]ä¸­ï¼š
        - å¼•ç”¨å¯¹è¯ä¸­çš„å…·ä½“è¯­å¥
        - åˆ†ææ˜¯å¦ç¬¦åˆæ ‡å‡†è¦æ±‚
        - æŒ‡å‡ºå­˜åœ¨/ç¼ºå¤±çš„è¦ç´ 
    2. åœ¨ã€è¯„åˆ†ã€‘ä¸­ç»™å‡ºè¯¥æ ‡å‡†å¾—åˆ†ï¼ˆ0 - æ»¡åˆ†ï¼‰

    å®Œæˆæ‰€æœ‰æ ‡å‡†è¯„ä¼°åï¼š
    1. è®¡ç®—æ€»åˆ†ï¼ˆæ»¡åˆ† 100 åˆ†ï¼‰
    2. åœ¨ã€æ€»ç»“ã€‘æ ‡ç­¾ä¸­ï¼š
        - æŒ‡å‡º 1 ä¸ªæœ€å…³é”®æ”¹è¿›ç‚¹
        - æ”¹è¿›ç‚¹åŒ…å«ï¼š
            * é—®é¢˜æè¿°ï¼ˆ20 å­—å†…ï¼‰
            * å…·ä½“å»ºè®®ï¼ˆ30 å­—å†…ï¼‰
            * ç¤ºèŒƒè¯æœ¯ï¼ˆå¯é€‰ï¼‰

    ä»¥MarkDownæ ¼å¼è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼š

    ### åˆ†æ
    #### æ ‡å‡† 1 - è‡ªæˆ‘ä»‹ç»
    - [åˆ†æå†…å®¹]
    **è¯„åˆ†ï¼š/30**
    #### æ ‡å‡† 2 - å®¢æˆ·éœ€æ±‚æ´å¯Ÿ
    - [åˆ†æå†…å®¹]
    **è¯„åˆ†ï¼š/20**
    #### æ ‡å‡† 3 - SPIN ç—›ç‚¹æŒ–æ˜
    - [åˆ†æå†…å®¹]
    **è¯„åˆ†ï¼š/15**
    #### æ ‡å‡† 4 - ä»·å€¼å±•ç¤ºèƒ½åŠ›
    - [åˆ†æå†…å®¹]
    **è¯„åˆ†ï¼š/15**
    #### æ ‡å‡† 5 - å†³ç­–æµç¨‹æŒæ¡
    - [åˆ†æå†…å®¹]
    **è¯„åˆ†ï¼š/10**
    #### æ ‡å‡† 6 - åç»­è·Ÿè¿›é“ºå«
    - [åˆ†æå†…å®¹]
    **è¯„åˆ†ï¼š/10**
    #### æ€»åˆ†
    **/100**

    æ€»ç»“
    1. æ”¹è¿›ç‚¹ï¼š[é—®é¢˜]
       å»ºè®®ï¼š[æ–¹æ¡ˆ] 
       ç¤ºä¾‹ï¼š[å¼•ç”¨å¯¹è¯è®°å½•çš„è¯æœ¯è¿›è¡Œä¿®æ”¹]

    ç°åœ¨å¼€å§‹é€é¡¹åˆ†æã€‚
    """
    llm = ChatOpenAI(
        openai_api_key="sk-OdCoqKCvctCJaPHUF2Ea9eF9C01940D8Aa7cB82889EaE165",
        openai_api_base="https://api.pumpkinaigc.online/v1",
        model_name="gpt-4o",
        temperature=0.7
    )
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ä»¥ä¸‹æ˜¯éœ€è¦åˆ†æçš„é€šè¯è®°å½•ï¼š\n\n{formatted_text}")
    ])
    try:
        response = llm(prompt.format_messages())
        analysis_text = response.content
        filtered_text = re.sub(r"(>?\s*Reasoning[\s\S]*?Reasoned for \d+\s*seconds\s*)", "", analysis_text, flags=re.IGNORECASE)
        return {
            "status": "success",
            "analysis": filtered_text,
            "formatted_text": formatted_text,
            "roles": roles
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"
        }

async def llm_workflow(conversation_text: str) -> dict:
    """
    é’ˆå¯¹æ¯ä¸ªè½¬å†™æ–‡ä»¶ï¼Œå…ˆè°ƒç”¨identify_rolesï¼Œå†è°ƒç”¨analyze_conversation_with_rolesï¼Œ
    å½¢æˆä¸€ä¸ªå®Œæ•´çš„LLMå·¥ä½œæµ
    """
    roles = await asyncio.to_thread(identify_roles, conversation_text)
    analysis_result = await asyncio.to_thread(analyze_conversation_with_roles, conversation_text, roles)
    return analysis_result

async def process_file(upload_result: Dict) -> Dict:
    """
    å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šè°ƒç”¨è½¬å†™APIã€è§£æç»“æœã€ä¿å­˜è½¬å†™æ–‡æœ¬å¹¶å¯åŠ¨LLMå·¥ä½œæµï¼ˆè§’è‰²è¯†åˆ«å’Œé€šè¯è®°å½•åˆ†æï¼‰
    """
    file_path = upload_result["file_path"]
    logging.debug(f"å¼€å§‹å¤„ç†æ–‡ä»¶ {file_path}")
    result = upload_result["result"]
    if 'content' in result and 'orderId' in result['content']:
        orderId = result['content']['orderId']
        logging.debug(f"è°ƒç”¨è½¬å†™ API å‰ï¼Œæ–‡ä»¶ {file_path}ï¼ŒorderId: {orderId}")
        transcription_result = await get_transcription_result_async(orderId)
        logging.debug(f"è½¬å†™ API è¿”å›ï¼Œæ–‡ä»¶ {file_path}")
        if 'content' in transcription_result:
            try:
                js_xunfei_result = json.loads(transcription_result['content']['orderResult'])
            except Exception as e:
                return {"file_path": file_path, "status": "error", "message": f"è§£æè½¬å†™ç»“æœå¤±è´¥: {e}"}
            content = []
            for result_one_vad_str in js_xunfei_result['lattice']:
                try:
                    js_result_one_vad = json.loads(result_one_vad_str['json_1best'])
                    content.extend(merge_result_for_one_vad(js_result_one_vad))
                except Exception as e:
                    logging.error(f"è§£æå•ä¸ªvadç»“æœé”™è¯¯: {e}")
            file_name = os.path.basename(file_path)
            output_file_path = f"{file_name}_output.txt"
            with open(output_file_path, 'w', encoding='utf-8') as f:
                for line in content:
                    f.write(line)
            with open(output_file_path, 'r', encoding='utf-8') as f:
                conversation_text = f.read()
            logging.debug(f"å¼€å§‹è°ƒç”¨LLMå·¥ä½œæµåˆ†æï¼Œæ–‡ä»¶ {file_path}")
            analysis_result = await llm_workflow(conversation_text)
            logging.debug(f"LLMå·¥ä½œæµåˆ†æå®Œæˆï¼Œæ–‡ä»¶ {file_path}")
            return {
                "file_path": file_path,
                "status": "success",
                "analysis_result": analysis_result,
                "output_file_path": output_file_path
            }
        else:
            return {"file_path": file_path, "status": "error", "message": "è½¬å†™ç»“æœæ ¼å¼é”™è¯¯"}
    else:
        return {"file_path": file_path, "status": "error", "message": "ä¸Šä¼ å¤±è´¥æˆ–è¿”å›æ ¼å¼é”™è¯¯"}

async def process_all_files(temp_files: List[str], progress_placeholder) -> List[Dict]:
    """
    å¼‚æ­¥å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼šå…ˆå¹¶å‘ä¸Šä¼ ï¼Œå†å¹¶å‘å¤„ç†è½¬å†™å’Œåˆ†æï¼Œæ¯å®Œæˆä¸€ä¸ªæ–‡ä»¶æ›´æ–°è¿›åº¦
    è¿›åº¦æ¡åˆ’åˆ†ï¼š
      ä¸Šä¼ é˜¶æ®µï¼š0 ~ 0.2
      æ–‡ä»¶å¤„ç†é˜¶æ®µï¼š0.2 ~ 0.8
    """
    progress_bar = progress_placeholder.progress(0)
    status_text = progress_placeholder.empty()
    phase_text = progress_placeholder.empty()

    # ä¸Šä¼ æ–‡ä»¶é˜¶æ®µ
    phase_text.markdown("**ğŸ“¤ æ­£åœ¨ä¸Šä¼ æ–‡ä»¶...**")
    logging.debug("å¼€å§‹å¹¶å‘ä¸Šä¼ æ–‡ä»¶")
    upload_results = await upload_files_async(temp_files)
    logging.debug("å®Œæˆæ–‡ä»¶ä¸Šä¼ ")
    phase_text.markdown("**ğŸ“¤ ä¸Šä¼ å®Œæˆï¼**")
    progress_bar.progress(0.2)

    # å¤„ç†æ–‡ä»¶é˜¶æ®µ
    phase_text.markdown("**ğŸ”„ æ­£åœ¨è½¬å†™å¹¶åˆ†ææ–‡ä»¶...**")
    tasks = [process_file(upload_result) for upload_result in upload_results]
    results = []
    total = len(tasks)
    count = 0
    for task in asyncio.as_completed(tasks):
        result = await task
        count += 1
        progress = 0.2 + 0.6 * (count / total)
        progress_bar.progress(progress)
        status_text.markdown(f"â³ å·²å®Œæˆ {count}/{total} ä¸ªæ–‡ä»¶")
        results.append(result)

    phase_text.markdown("**âœ… æ–‡ä»¶å¤„ç†å®Œæˆï¼**")
    progress_bar.progress(0.8)
    return results

def analyze_summary(all_analysis_results: List[Dict]) -> str:
    """
    å¯¹æ‰€æœ‰å¯¹è¯çš„åˆ†æç»“æœè¿›è¡Œæ±‡æ€»åˆ†æ
    """
    system_prompt = f"""
    ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é”€å”®åŸ¹è®­ä¸“å®¶ï¼Œéœ€è¦æ ¹æ®å½“æ—¥é”€å”®å¯¹è¯åˆ†ææŠ¥å‘Šè¿›è¡Œæ±‡æ€»åˆ†æï¼Œç”Ÿæˆä¸€ä»½ç»“æ„åŒ–çš„é”€å”®åˆ†ææŠ¥å‘Šã€‚

    è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¤„ç†æ•°æ®ï¼š
    1. æ•°æ®è§£æé˜¶æ®µï¼š
        - ä»”ç»†æå–æ‰€æœ‰é€šè¯è®°å½•çš„è¯„åˆ†æ•°æ®ï¼Œç¡®ä¿æ¯ä¸ªè¯„åˆ†éƒ½æœ‰å¯¹è¯æ–‡æœ¬æ”¯æ’‘ã€‚
        - è®¤çœŸè¯†åˆ«æ¯ä»½æŠ¥å‘Šä¸­çš„å…³é”®æ”¹è¿›ç‚¹ï¼Œé¿å…ä¸»è§‚è‡†æ–­ï¼Œä»…åŸºäºå¯¹è¯äº‹å®ã€‚
    2. è®¡ç®—åˆ†æé˜¶æ®µï¼š
        - è®¡ç®—å¹³å‡è¯„åˆ†ï¼Œç»“æœä¿ç•™ä¸¤ä½å°æ•°ã€‚
        - ç»Ÿè®¡é‡å¤å‡ºç°çš„æ”¹è¿›å»ºè®®é¢‘æ¬¡ã€‚
    3. å»ºè®®ç­›é€‰é˜¶æ®µï¼š
        - é€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„å‰3ä¸ªæ”¹è¿›é¢†åŸŸã€‚
        - ç¡®ä¿æ¯æ¡å»ºè®®æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
          a) åŸºäºè‡³å°‘3ä¸ªé€šè¯è®°å½•çš„å…±åŒé—®é¢˜ã€‚
          b) èšç„¦å¯é‡åŒ–çš„è¡Œä¸ºæ”¹è¿›ã€‚
          c) åŒ…å«å…·ä½“çš„æå‡æ–¹å‘ã€‚

    è¾“å‡ºè¦æ±‚ï¼š
    è¯·åœ¨[é”€å”®åˆ†ææŠ¥å‘Š]æ ‡ç­¾ä¸‹è¾“å‡ºä»¥ä¸‹å†…å®¹ï¼Œä»¥Markdownå½¢å¼å‘ˆç°ï¼š
    ### [é”€å”®åˆ†ææŠ¥å‘Š]
    1. **å¹³å‡è¯„åˆ†**ï¼šæ•°å€¼ç»“æœ
    2. **æ”¹è¿›å»ºè®®**ï¼š
        - æ¯æ¡å»ºè®®å•ç‹¬åˆ—å‡ºï¼Œé—®é¢˜æè¿°ç®€æ˜ï¼ˆä¸è¶…è¿‡25å­—ï¼‰ï¼Œæ”¹è¿›æªæ–½å…·ä½“å¯æ‰§è¡Œï¼Œæ•´ä½“æ§åˆ¶åœ¨50å­—å·¦å³ã€‚

    ç‰¹åˆ«æ³¨æ„ï¼š
    - ä¼˜å…ˆå¤„ç†å½±å“å®¢æˆ·è½¬åŒ–ç‡çš„è¦ç´ ã€‚
    - å»ºè®®éœ€åŒ…å«å¯è¡¡é‡çš„è¡Œä¸ºæŒ‡æ ‡ï¼Œé¿å…ä½¿ç”¨æ¨¡ç³Šæ€§è¡¨è¿°ã€‚
    - ä¿æŒå»ºè®®é—´çš„æ­£äº¤æ€§ï¼Œä¸é‡å¤è¦†ç›–ç›¸åŒç»´åº¦ã€‚
    - æ”¹è¿›å»ºè®®è¦å¯è®©é”€å”®å¯æ‰§è¡Œã€‚

    ### [é”€å”®åˆ†ææŠ¥å‘Š]
    1. **å¹³å‡è¯„åˆ†**ï¼š[åœ¨æ­¤å¡«å†™å¹³å‡è¯„åˆ†]
    2. **æ”¹è¿›å»ºè®®**ï¼š
        - [å»ºè®®1ï¼ŒæŒ‰ç…§è¦æ±‚æ ¼å¼ç¼–å†™]
        - [å»ºè®®2ï¼ŒæŒ‰ç…§è¦æ±‚æ ¼å¼ç¼–å†™]
        - [å»ºè®®3ï¼ŒæŒ‰ç…§è¦æ±‚æ ¼å¼ç¼–å†™]
    """
    llm = ChatOpenAI(
        openai_api_key="f465c1fc-481e-4668-bfa2-ec9187c2f1e4",
        openai_api_base="https://ark.cn-beijing.volces.com/api/v3",
        model_name="deepseek-r1-250120",
        temperature=0.7
    )

    all_analyses = []
    for idx, result in enumerate(all_analysis_results, 1):
        if result["status"] == "success" and result["analysis_result"].get("status") == "success":
            all_analyses.append(f"å¯¹è¯ {idx} çš„åˆ†æç»“æœï¼š\n{result['analysis_result']['analysis']}")

    combined_analyses = "\n\n".join(all_analyses)

    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ä»¥ä¸‹æ˜¯{len(all_analyses)}ä¸ªé”€å”®å¯¹è¯çš„åˆ†æç»“æœï¼Œè¯·è¿›è¡Œæ±‡æ€»åˆ†æï¼š\n\n{combined_analyses}")
    ])

    try:
        response = llm(prompt.format_messages())
        return response.content
    except Exception as e:
        return f"æ±‡æ€»åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

# Streamlitç•Œé¢
st.set_page_config(page_title="åˆ†æé€šè¯è®°å½•Demo", page_icon="ğŸ“")
st.title("åˆ†æé€šè¯è®°å½•ï¼ˆDemoï¼‰ğŸ“")

# åˆå§‹åŒ–session state
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'combined_report' not in st.session_state:
    st.session_state.combined_report = None
if 'summary_analysis' not in st.session_state:
    st.session_state.summary_analysis = None
if 'analysis_completed' not in st.session_state:
    st.session_state.analysis_completed = False  # ç”¨æ¥æ ‡è®°åˆ†ææ˜¯å¦å®Œæˆ
if 'contact_person' not in st.session_state:
    st.session_state.contact_person = ""  # ç”¨äºå­˜å‚¨è”ç³»äººä¿¡æ¯

# æ·»åŠ è”ç³»äººè¾“å…¥æ¡†
contact_person = st.text_input("è¯·è¾“å…¥æœ¬æ¬¡å¯¹æ¥å®¢æˆ·çš„è”ç³»äºº", value=st.session_state.contact_person)
if contact_person != st.session_state.contact_person:
    st.session_state.contact_person = contact_person

uploaded_files = st.file_uploader(
    "è¯·ä¸Šä¼ é€šè¯å½•éŸ³æ–‡ä»¶",
    type=['wav', 'mp3', 'm4a', 'ogg'],
    accept_multiple_files=True
)

if uploaded_files and not st.session_state.analysis_completed:
    st.write("å·²ä¸Šä¼ çš„æ–‡ä»¶:")
    for file in uploaded_files:
        st.write(f"- {file.name}")

    if st.button("å¼€å§‹åˆ†æ"):
        progress_placeholder = st.container()

        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°æœ¬åœ°ä¸´æ—¶æ–‡ä»¶å¤¹
        temp_files = []
        for uploaded_file in uploaded_files:
            temp_path = f"./temp_{uploaded_file.name}"
            with open(temp_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            temp_files.append(temp_path)

        try:
            results = asyncio.run(process_all_files(temp_files, progress_placeholder))
            st.session_state.analysis_results = results

            # ç”Ÿæˆæ±‡æ€»åˆ†æå¹¶ä¿å­˜ï¼ŒåŒæ—¶æ›´æ–°è¿›åº¦æ¡ï¼ˆæ±‡æ€»åˆ†æå  20%ï¼‰
            phase_text = progress_placeholder.empty()
            phase_text.markdown("**ğŸ”„ æ­£åœ¨ç”Ÿæˆæ±‡æ€»åˆ†æ...**")
            progress_bar = progress_placeholder.progress(0.9)
            st.session_state.summary_analysis = analyze_summary([res for res in results if res["status"] == "success"])
            progress_bar.progress(1.0)
            phase_text.markdown("**âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼**")

            # ç”Ÿæˆå®Œæ•´æŠ¥å‘Šå¹¶ä¿å­˜
            combined_report = ""
            for idx, res in enumerate(results, 1):
                if res["status"] == "success" and res["analysis_result"].get("status") == "success":
                    combined_report += f"\n\n{'=' * 50}\nå¯¹è¯è®°å½• {idx}ï¼š\n{'=' * 50}\n\n"
                    combined_report += res["analysis_result"]["formatted_text"]
                    combined_report += f"\n\n{'=' * 50}\nåˆ†æç»“æœ {idx}ï¼š\n{'=' * 50}\n\n"
                    combined_report += res["analysis_result"]["analysis"]

            combined_report += f"\n\n{'=' * 50}\næ±‡æ€»åˆ†ææŠ¥å‘Šï¼š\n{'=' * 50}\n\n"
            combined_report += st.session_state.summary_analysis
            st.session_state.combined_report = combined_report

            st.session_state.analysis_completed = True  # æ ‡è®°åˆ†æå®Œæˆ

        except Exception as e:
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    os.remove(temp_file)

if st.session_state.analysis_results:
    tab1, tab2, tab3 = st.tabs(["ğŸ“ æ‰€æœ‰å¯¹è¯è®°å½•", "ğŸ“Š æ‰€æœ‰åˆ†æç»“æœ", "ğŸ“ˆ æ±‡æ€»åˆ†æ"])

    with tab1:
        for idx, res in enumerate(st.session_state.analysis_results, 1):
            if res["status"] == "success":
                analysis_result = res["analysis_result"]
                if analysis_result.get("status") == "success":
                    st.markdown(f"### ğŸ“ å¯¹è¯è®°å½• {idx}")
                    if analysis_result["roles"].get("confidence", "low") != "high":
                        st.warning("âš ï¸ è¯¥å¯¹è¯çš„è§’è‰²è¯†åˆ«å¯ä¿¡åº¦ä¸é«˜ï¼Œè¯·æ ¸å®ã€‚")
                    st.markdown(f"**è§’è‰²è¯´æ˜ï¼š**")
                    st.markdown(f"- è¯´è¯è€…1 ({analysis_result['roles']['spk1']})")
                    st.markdown(f"- è¯´è¯è€…2 ({analysis_result['roles']['spk2']})")
                    st.markdown("**è¯¦ç»†å¯¹è¯ï¼š**")
                    st.markdown(analysis_result["formatted_text"])
                    st.markdown("---")

    with tab2:
        for idx, res in enumerate(st.session_state.analysis_results, 1):
            if res["status"] == "success":
                analysis_result = res["analysis_result"]
                if analysis_result.get("status") == "success":
                    file_name = os.path.basename(res["file_path"])
                    file_name = re.sub(r'^temp_', '', file_name)
                    file_name = os.path.splitext(file_name)[0]
                    with st.expander(f"ğŸ“Š {file_name} é€šè¯åˆ†æ"):
                        st.markdown(analysis_result["analysis"])
                        st.markdown("---")

    with tab3:
        st.markdown("### ğŸ“ˆ æ±‡æ€»åˆ†ææŠ¥å‘Š")
        st.markdown(st.session_state.summary_analysis)

    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½å®Œæ•´åˆ†ææŠ¥å‘Š",
            data=st.session_state.combined_report,
            file_name="complete_analysis_report.md",
            mime="text/plain"
        )

    with col2:
        def generate_excel_report():
            try:
                workbook = openpyxl.load_workbook("ç”µè¯å¼€æ‹“åˆ†æè¡¨.xlsx")
                worksheet = workbook.active
                file_names = []
                analysis_data = []
                for res in st.session_state.analysis_results:
                    if res["status"] == "success" and res["analysis_result"].get("status") == "success":
                        file_name = os.path.basename(res["file_path"])
                        file_name = re.sub(r'^temp_', '', file_name)
                        file_name = os.path.splitext(file_name)[0]
                        file_names.append(file_name)
                        analysis_text = res["analysis_result"]["analysis"]
                        score = ""
                        score_patterns = [
                            r'æ€»åˆ†\s*\n\s*####\s*(\d+)/100',
                            r'æ€»åˆ†\s*\n\s*æ€»åˆ†ï¼š\s*(\d+)/100',
                            r'æ€»åˆ†\s*\n\s*(\d+)/100',
                            r'æ€»åˆ†ï¼š\s*(\d+)/100',
                            r'æ€»åˆ†\s*(\d+)/100',
                            r'æ€»åˆ†ï¼š?\s*(\d+)',
                            r'####\s*æ€»åˆ†\s*\n\s*\*\*(\d+)/100\*\*',
                            r'æ€»åˆ†\s*\n\s*\*\*(\d+)/100\*\*',
                            r'\*\*(\d+)/100\*\*',
                            r'æ€»åˆ†\s*\n\s*(\d+)'
                        ]
                        for pattern in score_patterns:
                            score_match = re.search(pattern, analysis_text)
                            if score_match:
                                score = score_match.group(1)
                                break
                        if not score:
                            general_score_match = re.search(r'(\d+)/100', analysis_text)
                            if general_score_match:
                                score = general_score_match.group(1)
                        suggestion = ""
                        suggestion_patterns = [
                            r'å»ºè®®ï¼š\s*(.+?)(?:\n|$)',
                            r'å»ºè®®ï¼š\s*\*\*(.+?)\*\*',
                            r'å»ºè®®ï¼š\s*(.+?)\*\*',
                            r'å»ºè®®ï¼š\s*(.+)',
                            r'æ”¹è¿›ç‚¹ï¼š.+?\n\s*å»ºè®®ï¼š\s*(.+?)(?:\n|$)',
                            r'\*\*å»ºè®®\*\*ï¼š\s*(.+?)(?:\n|$)',
                            r'\*\*å»ºè®®\*\*ï¼š\s*(.+)',
                            r'æ€»ç»“\s*\n\s*\d+\.\s*æ”¹è¿›ç‚¹.+?\n\s*å»ºè®®ï¼š\s*(.+?)(?:\n|$)',
                            r'æ€»ç»“\s*\n\s*\d+\.\s*æ”¹è¿›ç‚¹.+?\n\s*\*\*å»ºè®®\*\*ï¼š\s*(.+?)(?:\n|$)',
                            r'æ€»ç»“\s*\n\s*\d+\.\s*æ”¹è¿›ç‚¹ï¼š.+?\n\s*- \*\*å»ºè®®\*\*ï¼š\s*(.+?)(?:\n|$)',
                            r'æ€»ç»“\s*\n\s*\d+\.\s*æ”¹è¿›ç‚¹ï¼š.+?\n\s*- å»ºè®®ï¼š\s*(.+?)(?:\n|$)',
                            r'å»ºè®®\s*(.+?)(?:\n|$)'
                        ]
                        for pattern in suggestion_patterns:
                            suggestion_match = re.search(pattern, analysis_text)
                            if suggestion_match:
                                suggestion = suggestion_match.group(1).strip()
                                suggestion = re.sub(r'\*\*(.+?)\*\*', r'\1', suggestion)
                                suggestion = re.sub(r'\*(.+?)\*', r'\1', suggestion)
                                break
                        if not suggestion:
                            summary_section = re.search(r'æ€»ç»“.*?(?:\n|$)(.*?)(?=##|\Z)', analysis_text, re.DOTALL)
                            if summary_section:
                                summary_text = summary_section.group(1)
                                dash_content = re.search(r'-\s*(.+?)(?:\n|$)', summary_text)
                                if dash_content:
                                    suggestion = dash_content.group(1).strip()
                                    suggestion = re.sub(r'\*\*(.+?)\*\*', r'\1', suggestion)
                                    suggestion = re.sub(r'\*(.+?)\*', r'\1', suggestion)
                        if not suggestion:
                            summary_match = re.search(r'æ€»ç»“.*?(?:\n|$)(.*?)(?=\n\n|\Z)', analysis_text, re.DOTALL)
                            if summary_match:
                                first_sentence = re.search(r'[^.!?ã€‚ï¼ï¼Ÿ]+[.!?ã€‚ï¼ï¼Ÿ]', summary_match.group(1))
                                if first_sentence:
                                    suggestion = first_sentence.group(0).strip()
                                    suggestion = re.sub(r'\*\*(.+?)\*\*', r'\1', suggestion)
                                    suggestion = re.sub(r'\*(.+?)\*', r'\1', suggestion)
                        analysis_data.append({"score": score, "suggestion": suggestion})
                column_indices = {}
                for col in range(1, worksheet.max_column + 1):
                    header = worksheet.cell(1, col).value
                    if header:
                        column_indices[header] = col
                for i, (name, data) in enumerate(zip(file_names, analysis_data)):
                    row = i + 2
                    if row <= worksheet.max_row:
                        if "å®¢æˆ·åç§°" in column_indices:
                            worksheet.cell(row, column_indices["å®¢æˆ·åç§°"]).value = name
                        if "è”ç³»äºº" in column_indices:
                            worksheet.cell(row, column_indices["è”ç³»äºº"]).value = st.session_state.contact_person
                        if "è¯„åˆ†" in column_indices and data["score"]:
                            try:
                                worksheet.cell(row, column_indices["è¯„åˆ†"]).value = int(data["score"])
                            except ValueError:
                                worksheet.cell(row, column_indices["è¯„åˆ†"]).value = data["score"]
                        if "é€šè¯ä¼˜åŒ–å»ºè®®" in column_indices and data["suggestion"]:
                            worksheet.cell(row, column_indices["é€šè¯ä¼˜åŒ–å»ºè®®"]).value = data["suggestion"]
                if st.session_state.summary_analysis:
                    avg_score = ""
                    avg_score_patterns = [
                        r'å¹³å‡è¯„åˆ†[^\d]*(\d+\.?\d*)',
                        r'å¹³å‡è¯„åˆ†ï¼š\s*(\d+\.?\d*)',
                        r'å¹³å‡[^\d]*(\d+\.?\d*)',
                        r'å¹³å‡åˆ†[^\d]*(\d+\.?\d*)'
                    ]
                    for pattern in avg_score_patterns:
                        avg_score_match = re.search(pattern, st.session_state.summary_analysis)
                        if avg_score_match:
                            avg_score = avg_score_match.group(1)
                            break
                    suggestions = []
                    list_items = re.findall(r'- (.+?)(?:\n|$)', st.session_state.summary_analysis)
                    if list_items:
                        suggestions.extend(list_items)
                    if not suggestions:
                        numbered_items = re.findall(r'\d+\.\s+(.+?)(?:\n|$)', st.session_state.summary_analysis)
                        if numbered_items:
                            suggestions.extend(numbered_items)
                    formatted_suggestions = "æ”¹è¿›å»ºè®®ï¼š\n"
                    for suggestion in suggestions:
                        clean_suggestion = re.sub(r'\*\*(.+?)\*\*', r'\1', suggestion)
                        clean_suggestion = re.sub(r'\*(.+?)\*', r'\1', clean_suggestion)
                        formatted_suggestions += f"- {clean_suggestion}\n"
                    summary_row = 32
                    for row in range(1, worksheet.max_row + 1):
                        cell_value = worksheet.cell(row, 1).value
                        if cell_value and "æ€»ç»“" in str(cell_value):
                            summary_row = row
                            break
                    if formatted_suggestions:
                        worksheet.cell(summary_row, 2).value = formatted_suggestions
                    total_score_col = None
                    for col in range(1, worksheet.max_column + 1):
                        cell_value = worksheet.cell(summary_row, col).value
                        if cell_value and "æ€»è¯„åˆ†" in str(cell_value):
                            total_score_col = col
                            break
                    if total_score_col and avg_score:
                        worksheet.cell(summary_row, total_score_col).value = f"æ€»è¯„åˆ†ï¼š\n{avg_score}"
                        worksheet.cell(summary_row, total_score_col).alignment = openpyxl.styles.Alignment(wrapText=True)
                output = BytesIO()
                workbook.save(output)
                output.seek(0)
                processed_data = output.getvalue()
                return processed_data
            except Exception as e:
                st.error(f"å¤„ç†Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                return None

        excel_data = generate_excel_report()
        if excel_data:
            st.download_button(
                label="ğŸ“Š ä¸‹è½½ç”µè¯å¼€æ‹“åˆ†æè¡¨",
                data=excel_data,
                file_name="ç”µè¯å¼€æ‹“åˆ†æè¡¨_å·²å¡«å†™.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
