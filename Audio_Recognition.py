import os
import json
import time
import base64
import hashlib
import hmac
import urllib
import asyncio
import aiohttp
import logging
from typing import List, Dict
from config import XFYUN_CONFIG
from LLM_Workflow import llm_workflow

def get_signa(appid: str, secret_key: str, ts: str) -> str:
    """
    ç”Ÿæˆè®¯é£APIè¯·æ±‚ç­¾å
    
    Args:
        appid: åº”ç”¨ID
        secret_key: å¯†é’¥
        ts: æ—¶é—´æˆ³
        
    Returns:
        str: ç”Ÿæˆçš„ç­¾å
    """
    m2 = hashlib.md5()
    m2.update((appid + ts).encode('utf-8'))
    md5 = m2.hexdigest()
    md5 = bytes(md5, encoding='utf-8')
    signa = hmac.new(secret_key.encode('utf-8'), md5, hashlib.sha1).digest()
    signa = base64.b64encode(signa)
    return str(signa, 'utf-8')

async def upload_file_async(session: aiohttp.ClientSession, file_path: str) -> Dict:
    """å¼‚æ­¥ä¸Šä¼ å•ä¸ªæ–‡ä»¶åˆ°è®¯é£è¯­éŸ³è¯†åˆ«æœåŠ¡"""
    if not os.path.exists(file_path):
        return {
            "file_path": file_path,
            "status": "error",
            "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
        }

    ts = str(int(time.time()))
    signa = get_signa(XFYUN_CONFIG["appid"], XFYUN_CONFIG["secret_key"], ts)
    file_len = os.path.getsize(file_path)
    file_name = os.path.basename(file_path)
    param_dict = {
        'appId': XFYUN_CONFIG["appid"],
        'signa': signa,
        'ts': ts,
        'fileSize': file_len,
        'fileName': file_name,
        'duration': "200",
        'roleNum': 2,
        'roleType': 1
    }
    url = XFYUN_CONFIG["lfasr_host"] + XFYUN_CONFIG["api_upload"] + "?" + urllib.parse.urlencode(param_dict)
    with open(file_path, 'rb') as f:
        data = f.read()
    async with session.post(url, headers={"Content-type": "application/json"}, data=data) as response:
        result = await response.json()
        logging.debug(f"ä¸Šä¼ æ–‡ä»¶ {file_name} è¿”å›ç»“æœï¼š{result}")
        return {"file_path": file_path, "result": result}

async def upload_files_async(file_paths: List[str]) -> List[Dict]:
    """å¹¶å‘ä¸Šä¼ å¤šä¸ªæ–‡ä»¶"""
    async with aiohttp.ClientSession() as session:
        tasks = [upload_file_async(session, file_path) for file_path in file_paths]
        return await asyncio.gather(*tasks)

async def get_transcription_result_async(orderId: str) -> Dict:
    """
    å¼‚æ­¥è·å–è½¬å†™ç»“æœ
    
    Args:
        orderId: è®¯é£APIè¿”å›çš„è®¢å•ID
        
    Returns:
        Dict: è½¬å†™ç»“æœ
    """
    ts = str(int(time.time()))
    signa = get_signa(XFYUN_CONFIG["appid"], XFYUN_CONFIG["secret_key"], ts)
    param_dict = {
        'appId': XFYUN_CONFIG["appid"],
        'signa': signa,
        'ts': ts,
        'orderId': orderId,
        'resultType': "transfer,predict"
    }
    url = XFYUN_CONFIG["lfasr_host"] + XFYUN_CONFIG["api_get_result"] + "?" + urllib.parse.urlencode(param_dict)
    status = 3
    async with aiohttp.ClientSession() as session:
        while status == 3:
            async with session.post(url, headers={"Content-type": "application/json"}) as response:
                result = await response.json()
            status = result['content']['orderInfo']['status']
            logging.debug(f"è½¬å†™APIè°ƒç”¨è¿”å›çŠ¶æ€: {status} (orderId: {orderId})")
            if status == 4:
                break
            await asyncio.sleep(5)
    return result

def merge_result_for_one_vad(result_vad: Dict) -> List[str]:
    """
    è§„èŒƒåŒ–JSONæ–‡ä»¶ä¸ºå¯è¯»æ–‡æœ¬
    
    Args:
        result_vad: å•ä¸ªVADç»“æœ
        
    Returns:
        List[str]: å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨
    """
    content = []
    for rt_dic in result_vad['st']['rt']:
        spk_str = 'spk' + str(3 - int(result_vad['st']['rl'])) + '##'
        for st_dic in rt_dic['ws']:
            for cw_dic in st_dic['cw']:
                for w in cw_dic['w']:
                    spk_str += w
        spk_str += '\n'
        content.append(spk_str)
    return content

async def process_file(upload_result: Dict) -> Dict:
    """
    å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šè°ƒç”¨è½¬å†™APIã€è§£æç»“æœã€ä¿å­˜è½¬å†™æ–‡æœ¬å¹¶å¯åŠ¨LLMå·¥ä½œæµ
    
    Args:
        upload_result: ä¸Šä¼ ç»“æœ
        
    Returns:
        Dict: å¤„ç†ç»“æœï¼ŒåŒ…å«è½¬å†™æ–‡æœ¬å’Œåˆ†æç»“æœ
    """
    file_path = upload_result["file_path"]
    logging.debug(f"å¼€å§‹å¤„ç†æ–‡ä»¶ {file_path}")
    result = upload_result["result"]
    if 'content' in result and 'orderId' in result['content']:
        orderId = result['content']['orderId']
        logging.debug(f"è°ƒç”¨è½¬å†™ API å‰ï¼Œæ–‡ä»¶ {file_path}ï¼ŒorderId: {orderId}")
        transcription_result = await get_transcription_result_async(orderId)
        logging.debug(f"è½¬å†™ API è¿”å›ï¼Œæ–‡ä»¶ {file_path}")
        if 'content' in transcription_result:
            try:
                js_xunfei_result = json.loads(transcription_result['content']['orderResult'])
            except Exception as e:
                return {"file_path": file_path, "status": "error", "message": f"è§£æè½¬å†™ç»“æœå¤±è´¥: {e}"}
            content = []
            for result_one_vad_str in js_xunfei_result['lattice']:
                try:
                    js_result_one_vad = json.loads(result_one_vad_str['json_1best'])
                    content.extend(merge_result_for_one_vad(js_result_one_vad))
                except Exception as e:
                    logging.error(f"è§£æå•ä¸ªvadç»“æœé”™è¯¯: {e}")
            file_name = os.path.basename(file_path)
            output_file_path = f"{file_name}_output.txt"
            with open(output_file_path, 'w', encoding='utf-8') as f:
                for line in content:
                    f.write(line)
            
            with open(output_file_path, 'r', encoding='utf-8') as f:
                conversation_text = f.read()
            
            # è°ƒç”¨LLMå·¥ä½œæµè¿›è¡Œåˆ†æ
            logging.debug(f"å¼€å§‹è°ƒç”¨LLMå·¥ä½œæµåˆ†æï¼Œæ–‡ä»¶ {file_path}")
            analysis_result = await llm_workflow(conversation_text)
            logging.debug(f"LLMå·¥ä½œæµåˆ†æå®Œæˆï¼Œæ–‡ä»¶ {file_path}")
            
            return {
                "file_path": file_path,
                "status": "success",
                "analysis_result": analysis_result,
                "conversation_text": conversation_text,
                "output_file_path": output_file_path
            }
        else:
            return {"file_path": file_path, "status": "error", "message": "è½¬å†™ç»“æœæ ¼å¼é”™è¯¯"}
    else:
        return {"file_path": file_path, "status": "error", "message": "ä¸Šä¼ å¤±è´¥æˆ–è¿”å›æ ¼å¼é”™è¯¯"}

async def process_all_files(temp_files: List[str], progress_placeholder) -> List[Dict]:
    """
    å¼‚æ­¥å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼šå…ˆå¹¶å‘ä¸Šä¼ ï¼Œå†å¹¶å‘å¤„ç†è½¬å†™å’Œåˆ†æï¼Œæ¯å®Œæˆä¸€ä¸ªæ–‡ä»¶æ›´æ–°è¿›åº¦
    è¿›åº¦æ¡åˆ’åˆ†ï¼š
      ä¸Šä¼ é˜¶æ®µï¼š0 ~ 0.2
      æ–‡ä»¶å¤„ç†é˜¶æ®µï¼š0.2 ~ 0.8
      
    Args:
        temp_files: ä¸´æ—¶æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        progress_placeholder: Streamlitè¿›åº¦æ˜¾ç¤ºå®¹å™¨
        
    Returns:
        List[Dict]: å¤„ç†ç»“æœåˆ—è¡¨
    """
    progress_bar = progress_placeholder.progress(0)
    status_text = progress_placeholder.empty()
    phase_text = progress_placeholder.empty()

    # ä¸Šä¼ æ–‡ä»¶é˜¶æ®µ
    phase_text.markdown("**ğŸ“¤ æ­£åœ¨ä¸Šä¼ æ–‡ä»¶...**")
    logging.debug("å¼€å§‹å¹¶å‘ä¸Šä¼ æ–‡ä»¶")
    upload_results = await upload_files_async(temp_files)
    logging.debug("å®Œæˆæ–‡ä»¶ä¸Šä¼ ")
    phase_text.markdown("**ğŸ“¤ ä¸Šä¼ å®Œæˆï¼**")
    progress_bar.progress(0.2)

    # å¤„ç†æ–‡ä»¶é˜¶æ®µ
    phase_text.markdown("**ğŸ”„ æ­£åœ¨è½¬å†™æ–‡ä»¶...**")
    tasks = [process_file(upload_result) for upload_result in upload_results]
    results = []
    total = len(tasks)
    count = 0
    for task in asyncio.as_completed(tasks):
        result = await task
        count += 1
        progress = 0.2 + 0.6 * (count / total)
        progress_bar.progress(progress)
        status_text.markdown(f"â³ å·²å®Œæˆ {count}/{total} ä¸ªæ–‡ä»¶è½¬å†™")
        results.append(result)

    phase_text.markdown("**âœ… æ–‡ä»¶è½¬å†™å®Œæˆï¼**")
    progress_bar.progress(0.8)
    return results 