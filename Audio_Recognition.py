import os
import json
import time
import uuid
import datetime
import copy
import asyncio
import aiohttp
import logging
from typing import List, Dict, Any, Optional
import tos
from LLM_Workflow import llm_workflow
from config import VOLCANO_CONFIG  # ä»configå¯¼å…¥ç«å±±å¼•æ“é…ç½®

async def upload_to_tos_async(file_path: str) -> str:
    """
    å¼‚æ­¥å°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ°TOSå¹¶è·å–URL
    
    Args:
        file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        
    Returns:
        str: æ–‡ä»¶åœ¨TOSä¸Šçš„URL
    """
    logging.debug(f"å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°TOS: {file_path}")
    
    # ç”±äºtosåº“ä¸æ”¯æŒå¼‚æ­¥æ“ä½œï¼Œä½¿ç”¨run_in_executoråœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
    return await asyncio.to_thread(upload_to_tos_sync, file_path)
    
def upload_to_tos_sync(local_file_path: str) -> str:
    """
    åŒæ­¥å°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ°TOSï¼ˆåœ¨å¼‚æ­¥å‡½æ•°ä¸­é€šè¿‡çº¿ç¨‹æ± è°ƒç”¨ï¼‰
    
    Args:
        local_file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
        
    Returns:
        str: æ–‡ä»¶åœ¨TOSä¸Šçš„URL
    """
    # TOSå‡­è¯ä¿¡æ¯
    ak = VOLCANO_CONFIG["tos"]["ak"]
    sk = VOLCANO_CONFIG["tos"]["sk"]
    endpoint = VOLCANO_CONFIG["tos"]["endpoint"]
    region = VOLCANO_CONFIG["tos"]["region"]
    bucket_name = VOLCANO_CONFIG["tos"]["bucket_name"]
    
    logging.debug("åˆ›å»º TOS å®¢æˆ·ç«¯...")
    # åˆ›å»ºå®¢æˆ·ç«¯
    client = tos.TosClientV2(ak, sk, endpoint, region)
    
    # ç”Ÿæˆå”¯ä¸€çš„å¯¹è±¡é”®åï¼ˆä½¿ç”¨æ–‡ä»¶åŸå§‹åç§°+æ—¶é—´æˆ³ï¼‰
    file_name = os.path.basename(local_file_path)
    file_ext = os.path.splitext(file_name)[1]
    current_time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    object_key = f"{os.path.splitext(file_name)[0]}-{current_time}{file_ext}"
    
    try:
        # ä¸Šä¼ å¯¹è±¡
        logging.debug(f"ä¸Šä¼ å¯¹è±¡: {object_key}...")
        with open(local_file_path, 'rb') as f:
            resp = client.put_object(bucket_name, object_key, content=f)
        logging.debug(f"ä¸Šä¼ å¯¹è±¡å“åº”çŠ¶æ€ç : {resp.status_code}")
        
        # ä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•ç”Ÿæˆå…¬å…±URL
        # æ–¹æ³•1ï¼šè®¾ç½®å¯¹è±¡çš„ACLä¸ºpublic-read
        try:
            client.put_object_acl(bucket_name, object_key, acl="public-read")
            file_url = f"https://{bucket_name}.{endpoint}/{object_key}"
            logging.debug(f"å…¬å…±URL: {file_url}")
            return file_url
        except Exception as acl_error:
            logging.error(f"è®¾ç½®å¯¹è±¡ACLå¤±è´¥: {acl_error}")
            
            # æ–¹æ³•2ï¼šå°è¯•ä½¿ç”¨ç­¾åURL
            try:
                # ä½¿ç”¨ç­¾åURLå·¥å…·ç±»ç­¾åURL
                current_time = int(time.time())
                expiration = current_time + 24 * 60 * 60  # 24å°æ—¶åè¿‡æœŸ
                
                # è¿™é‡Œé’ˆå¯¹ä¸åŒç‰ˆæœ¬çš„TOS SDKæä¾›å‡ ç§å¯èƒ½çš„è°ƒç”¨æ–¹å¼
                try:
                    # å°è¯•ä½¿ç”¨ç­¾åURL
                    from tos.enum import HttpMethodEnum
                    signed_url = client.pre_signed_url(HttpMethodEnum.GET, bucket_name, object_key, expires=expiration)
                except ImportError:
                    try:
                        # å°è¯•ä½¿ç”¨å…¶ä»–å¯èƒ½çš„æ–¹æ³•
                        signed_url = client.get_presigned_url(bucket_name, object_key, expires=expiration)
                    except:
                        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
                        signed_url = client.generate_presigned_url(bucket_name, object_key, expiration)
                
                logging.debug(f"ç­¾åURL: {signed_url}")
                return signed_url
            except Exception as sign_error:
                logging.error(f"ç”Ÿæˆç­¾åURLå¤±è´¥: {sign_error}")
                
                # æ–¹æ³•3ï¼šå¦‚æœä»¥ä¸Šæ–¹æ³•éƒ½å¤±è´¥ï¼Œä½¿ç”¨ä¸´æ—¶å…¬å¼€URL
                temp_url = f"https://{bucket_name}.{endpoint}/{object_key}"
                logging.warning(f"æ— æ³•ç”Ÿæˆæ­£ç¡®çš„ç­¾åURLï¼Œä½¿ç”¨æ™®é€šURL: {temp_url}")
                logging.warning(f"è¯·ç¡®ä¿è¯¥å­˜å‚¨æ¡¶æœ‰å…¬å…±è¯»å–æƒé™ï¼Œå¦åˆ™è½¬å†™æœåŠ¡å¯èƒ½æ— æ³•è®¿é—®")
                return temp_url
    except Exception as e:
        logging.error(f"ä¸Šä¼ æ–‡ä»¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise

async def submit_task_async(session: aiohttp.ClientSession, file_url: str) -> Dict[str, Any]:
    """
    å¼‚æ­¥æäº¤è¯­éŸ³è½¬å†™ä»»åŠ¡
    
    Args:
        session: aiohttpä¼šè¯
        file_url: æ–‡ä»¶URL
        
    Returns:
        Dict: åŒ…å«task_idå’Œx_tt_logidçš„å­—å…¸
    """
    logging.debug(f"å¼€å§‹æäº¤è½¬å†™ä»»åŠ¡ï¼Œæ–‡ä»¶URL: {file_url}")
    submit_url = "https://openspeech.bytedance.com/api/v3/auc/bigmodel/submit"

    task_id = str(uuid.uuid4())

    headers = {
        "X-Api-App-Key": VOLCANO_CONFIG["appid"],
        "X-Api-Access-Key": VOLCANO_CONFIG["token"],
        "X-Api-Resource-Id": "volc.bigasr.auc",
        "X-Api-Request-Id": task_id,
        "X-Api-Sequence": "-1"
    }

    request = {
        "user": {
            "uid": "fake_uid"
        },
        "audio": {
            "url": file_url,
            "format": "mp3",  # æ ¹æ®å®é™…éŸ³é¢‘æ ¼å¼è°ƒæ•´
            "codec": "raw",
            "rate": 16000,
            "bits": 16,
            "channel": 1      # å¦‚æœæ˜¯åŒå£°é“éŸ³é¢‘ï¼Œæ”¹ä¸º2
        },
        "request": {
            "model_name": "bigmodel",
            "enable_itn": True,       # å¯ç”¨æ–‡æœ¬è§„èŒƒåŒ–
            "enable_punc": True,      # å¯ç”¨æ ‡ç‚¹
            "enable_ddc": True,       # å¯ç”¨è¯­ä¹‰é¡ºæ»‘
            "show_utterances": True,  # è¾“å‡ºè¯­éŸ³åœé¡¿ã€åˆ†å¥ã€åˆ†è¯ä¿¡æ¯
            "enable_speaker_info": True,  # å¯ç”¨è¯´è¯äººèšç±»åˆ†ç¦»
            "vad_segment": True,      # ä½¿ç”¨vadåˆ†å¥
            "corpus": {
                "correct_table_name": "",
                "context": ""
            }
        }
    }
    
    logging.debug(f'æäº¤è½¬å†™ä»»åŠ¡ï¼Œä»»åŠ¡ID: {task_id}')
    try:
        async with session.post(submit_url, data=json.dumps(request), headers=headers) as response:
            # æ£€æŸ¥å“åº”å¤´
            if 'X-Api-Status-Code' in response.headers and response.headers["X-Api-Status-Code"] == "20000000":
                logging.debug(f'æäº¤ä»»åŠ¡å“åº”çŠ¶æ€ç : {response.headers["X-Api-Status-Code"]}')
                logging.debug(f'æäº¤ä»»åŠ¡å“åº”æ¶ˆæ¯: {response.headers["X-Api-Message"]}')
                x_tt_logid = response.headers.get("X-Tt-Logid", "")
                logging.debug(f'æäº¤ä»»åŠ¡æ—¥å¿—ID: {x_tt_logid}')
                return {"task_id": task_id, "x_tt_logid": x_tt_logid}
            else:
                error_msg = f'æäº¤ä»»åŠ¡å¤±è´¥ï¼Œå“åº”å¤´ä¿¡æ¯: {response.headers}'
                logging.error(error_msg)
                raise Exception(error_msg)
    except Exception as e:
        logging.error(f"æäº¤è½¬å†™ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

async def query_task_async(session: aiohttp.ClientSession, task_id: str, x_tt_logid: str) -> Dict[str, Any]:
    """
    å¼‚æ­¥æŸ¥è¯¢è½¬å†™ä»»åŠ¡çŠ¶æ€
    
    Args:
        session: aiohttpä¼šè¯
        task_id: ä»»åŠ¡ID
        x_tt_logid: æ—¥å¿—ID
    
    Returns:
        Dict: æŸ¥è¯¢ç»“æœ
    """
    query_url = "https://openspeech.bytedance.com/api/v3/auc/bigmodel/query"

    headers = {
        "X-Api-App-Key": VOLCANO_CONFIG["appid"],
        "X-Api-Access-Key": VOLCANO_CONFIG["token"],
        "X-Api-Resource-Id": "volc.bigasr.auc",
        "X-Api-Request-Id": task_id,
        "X-Tt-Logid": x_tt_logid  # å›ºå®šä¼ é€’ x-tt-logid
    }

    async with session.post(query_url, data=json.dumps({}), headers=headers) as response:
        if 'X-Api-Status-Code' in response.headers:
            status_code = response.headers["X-Api-Status-Code"]
            logging.debug(f'æŸ¥è¯¢ä»»åŠ¡å“åº”çŠ¶æ€ç : {status_code}')
            logging.debug(f'æŸ¥è¯¢ä»»åŠ¡å“åº”æ¶ˆæ¯: {response.headers["X-Api-Message"]}')
            logging.debug(f'æŸ¥è¯¢ä»»åŠ¡æ—¥å¿—ID: {response.headers["X-Tt-Logid"]}')
            
            result = {
                "status_code": status_code,
                "message": response.headers["X-Api-Message"],
                "data": None
            }
            
            if status_code == "20000000":  # ä»»åŠ¡å®Œæˆ
                # è·å–å“åº”ä½“å†…å®¹
                result["data"] = await response.json()
            
            return result
        else:
            error_msg = f'æŸ¥è¯¢ä»»åŠ¡å¤±è´¥ï¼Œå“åº”å¤´ä¿¡æ¯: {response.headers}'
            logging.error(error_msg)
            raise Exception(error_msg)

def process_transcription_result(result_json: Dict[str, Any]) -> Dict[str, Any]:
    """
    å¤„ç†è½¬å†™ç»“æœï¼Œå»é™¤wordså­—æ®µå¹¶è¿”å›å¤„ç†åçš„ç»“æœ
    
    Args:
        result_json: åŸå§‹è½¬å†™ç»“æœJSON
        
    Returns:
        Dict: å¤„ç†åçš„è½¬å†™ç»“æœï¼ˆæ— wordså­—æ®µï¼‰
    """
    # åˆ›å»ºç»“æœçš„æ·±æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    processed_result = copy.deepcopy(result_json)
    
    # æ£€æŸ¥å¹¶å¤„ç†utteranceså­—æ®µ
    if 'result' in processed_result and 'utterances' in processed_result['result']:
        for utterance in processed_result['result']['utterances']:
            if 'words' in utterance:
                del utterance['words']  # åˆ é™¤wordså­—æ®µ
    
    return processed_result

def save_to_txt(result_json: Dict[str, Any], output_file: str) -> None:
    """
    å°†è½¬å†™ç»“æœä¿å­˜ä¸ºtxtæ–‡ä»¶
    
    Args:
        result_json: è½¬å†™ç»“æœJSON
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        # å†™å…¥å®Œæ•´æ–‡æœ¬
        if 'result' in result_json and 'text' in result_json['result']:
            f.write("ã€å®Œæ•´æ–‡æœ¬ã€‘\n")
            f.write(result_json['result']['text'])
            f.write("\n\n")
        
        # å†™å…¥åˆ†å¥å’Œè¯´è¯äººä¿¡æ¯
        if 'result' in result_json and 'utterances' in result_json['result']:
            f.write("ã€åˆ†å¥ä¿¡æ¯ã€‘\n")
            for i, utterance in enumerate(result_json['result']['utterances']):
                speaker = utterance.get('additions', {}).get('speaker', 'æœªçŸ¥')
                start_time = utterance.get('start_time', 0) / 1000  # æ¯«ç§’è½¬ç§’
                end_time = utterance.get('end_time', 0) / 1000
                text = utterance.get('text', '')
                
                f.write(f"è¯´è¯äºº {speaker} [{start_time:.2f}s-{end_time:.2f}s]: {text}\n")
        
        # å†™å…¥éŸ³é¢‘ä¿¡æ¯
        if 'audio_info' in result_json:
            f.write("\nã€éŸ³é¢‘ä¿¡æ¯ã€‘\n")
            duration = result_json['audio_info'].get('duration', 0) / 1000  # æ¯«ç§’è½¬ç§’
            f.write(f"æ€»æ—¶é•¿: {duration:.2f}ç§’\n")
        
        # æ·»åŠ è®¯é£æ ¼å¼çš„è½¬å†™ç»“æœç”¨äºLLMå¤„ç†
        if 'result' in result_json and 'utterances' in result_json['result']:
            speakers = {}
            # å…ˆå°†è¯´è¯äººIDæ˜ å°„åˆ°è¯´è¯äººåºå·ï¼ˆspk1, spk2ï¼‰
            for utterance in result_json['result']['utterances']:
                speaker_id = utterance.get('additions', {}).get('speaker', '1')
                if speaker_id not in speakers:
                    speakers[speaker_id] = f"spk{len(speakers) + 1}"
            
            # ç”Ÿæˆè®¯é£APIå…¼å®¹æ ¼å¼
            for utterance in result_json['result']['utterances']:
                speaker_id = utterance.get('additions', {}).get('speaker', '1')
                spk_prefix = speakers.get(speaker_id, "spk1")
                text = utterance.get('text', '')
                f.write(f"{spk_prefix}##{text}\n")

async def process_file(file_path: str) -> Dict[str, Any]:
    """
    å¼‚æ­¥å¤„ç†å•ä¸ªæ–‡ä»¶ï¼šä¸Šä¼ ã€æäº¤è½¬å†™ä»»åŠ¡ã€æŸ¥è¯¢ç»“æœï¼Œä¿å­˜è½¬å†™æ–‡æœ¬å¹¶å¯åŠ¨LLMå·¥ä½œæµ
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
    
    Returns:
        Dict: å¤„ç†ç»“æœï¼ŒåŒ…å«è½¬å†™æ–‡æœ¬å’Œåˆ†æç»“æœ
    """
    logging.debug(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
    
    if not os.path.exists(file_path):
        return {
            "file_path": file_path,
            "status": "error",
            "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
        }
    
    try:
        # 1. ä¸Šä¼ æ–‡ä»¶åˆ°TOS
        file_url = await upload_to_tos_async(file_path)
        logging.debug(f"æ–‡ä»¶å·²ä¸Šä¼ åˆ°TOS: {file_url}")
        
        # 2. æäº¤è½¬å†™ä»»åŠ¡
        async with aiohttp.ClientSession() as session:
            submit_result = await submit_task_async(session, file_url)
            task_id = submit_result["task_id"]
            x_tt_logid = submit_result["x_tt_logid"]
            
            # 3. è½®è¯¢æŸ¥è¯¢ä»»åŠ¡ç»“æœ
            while True:
                query_result = await query_task_async(session, task_id, x_tt_logid)
                status_code = query_result["status_code"]
                
                if status_code == "20000000":  # ä»»åŠ¡å®Œæˆ
                    logging.debug("è½¬å†™ç»“æœè·å–æˆåŠŸ!")
                    result_json = query_result["data"]
                    break
                elif status_code != "20000001" and status_code != "20000002":  # ä»»åŠ¡å¤±è´¥
                    error_msg = f"è½¬å†™å¤±è´¥: {query_result['message']}"
                    logging.error(error_msg)
                    return {
                        "file_path": file_path,
                        "status": "error",
                        "message": error_msg
                    }
                else:  # ä»»åŠ¡å¤„ç†ä¸­
                    logging.debug(f"ä»»åŠ¡å¤„ç†ä¸­ï¼ŒçŠ¶æ€ç : {status_code}ï¼Œç­‰å¾…5ç§’åé‡è¯•...")
                    await asyncio.sleep(5)
        
        # 4. å¤„ç†è½¬å†™ç»“æœ
        processed_result = process_transcription_result(result_json)
        
        # 5. ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆåŸºäºåŸæ–‡ä»¶åï¼‰
        file_name = os.path.basename(file_path)
        output_file_path = f"{file_name}_output.txt"
        
        # 6. ä¿å­˜å¤„ç†åçš„ç»“æœåˆ°txtæ–‡ä»¶
        save_to_txt(processed_result, output_file_path)
        logging.debug(f"å·²å°†è½¬å†™ç»“æœä¿å­˜è‡³: {output_file_path}")
        
        # 7. è¯»å–è½¬å†™æ–‡æœ¬ï¼Œå‡†å¤‡è¿›è¡ŒLLMåˆ†æ
        with open(output_file_path, 'r', encoding='utf-8') as f:
            conversation_text = f.read()
        
        # 8. è°ƒç”¨LLMå·¥ä½œæµè¿›è¡Œåˆ†æ
        logging.debug(f"å¼€å§‹è°ƒç”¨LLMå·¥ä½œæµåˆ†æï¼Œæ–‡ä»¶ {file_path}")
        analysis_result = await llm_workflow(conversation_text)
        logging.debug(f"LLMå·¥ä½œæµåˆ†æå®Œæˆï¼Œæ–‡ä»¶ {file_path}")
        
        return {
            "file_path": file_path,
            "status": "success",
            "analysis_result": analysis_result,
            "conversation_text": conversation_text,
            "output_file_path": output_file_path
        }
    
    except Exception as e:
        logging.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {
            "file_path": file_path,
            "status": "error", 
            "message": str(e)
        }

async def process_all_files(temp_files: List[str], progress_placeholder) -> List[Dict[str, Any]]:
    """
    å¼‚æ­¥å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼šå¹¶å‘å¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼Œæ¯å®Œæˆä¸€ä¸ªæ–‡ä»¶æ›´æ–°è¿›åº¦
    è¿›åº¦æ¡åˆ’åˆ†ï¼š
      æ–‡ä»¶å¤„ç†é˜¶æ®µï¼š0 ~ 1.0
      
    Args:
        temp_files: ä¸´æ—¶æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        progress_placeholder: Streamlitè¿›åº¦æ˜¾ç¤ºå®¹å™¨
        
    Returns:
        List[Dict]: å¤„ç†ç»“æœåˆ—è¡¨
    """
    progress_bar = progress_placeholder.progress(0)
    status_text = progress_placeholder.empty()
    phase_text = progress_placeholder.empty()

    # å¤„ç†æ–‡ä»¶é˜¶æ®µ
    phase_text.markdown("**ğŸ”„ æ­£åœ¨è½¬å†™æ–‡ä»¶...**")
    tasks = [process_file(file_path) for file_path in temp_files]
    results = []
    total = len(tasks)
    count = 0
    
    for task in asyncio.as_completed(tasks):
        result = await task
        count += 1
        progress = count / total
        progress_bar.progress(progress)
        status_text.markdown(f"â³ å·²å®Œæˆ {count}/{total} ä¸ªæ–‡ä»¶è½¬å†™")
        results.append(result)

    phase_text.markdown("**âœ… æ–‡ä»¶è½¬å†™å®Œæˆï¼**")
    progress_bar.progress(1.0)
    return results 